---
title: About
---
## What is MLeRP?
Say we have a researcher who is a domain expert in a field and is just discovering that machine learning algorithms might be appropriate to accelerate their work. 

<p style="text-align: center;">![](./images/before-after.png)</p>

They explore online for potential approaches and even find some tutorials that use interactive environments, but quickly discover that dealing with large datasets requires more compute than their laptop can handle so they need to move to a HPC system. 

<p style="text-align: center;">![](./images/notebook-cat.png)</p>

While desktop sessions are available they might have prohibitively long queues and have low utilisation of the underlying hardware. They could submit a job to the HPC queue but this leads to long times between iterations. At this stage the researcher is still discovering things about their data, how to clean it and how to analyse it. 

<p style="text-align: center;">![](./images/hpc-cat.png)</p>

They need an interactive environment so they can develop and debug their algorithms and the algorithms, but still need access to high powered acceleration so they can process their dataset. At the same time we want to improve utilisation of the hardware so that we can serve more users and reduce wait times.

MLeRP was built to be a middle ground that has the interactivity of a notebook with the power of a HPC environment that can share valuable resources between other users while code isnâ€™t being executed. 

<p style="text-align: center;">![](./images/mlerp-rings.png)</p>
<p style="text-align: center;">![](./images/captain-research-cloud.png)</p>

Now our new researcher can easily import the same code that they were using on their laptops or with online notebook services straight into a HPC environment without the need to convert it first into a SBATCH script, wait in long HPC queues or load modules for dependencies. 

MLeRP is split into two regions which users can choose between - one based in Monash (Melbourne, Victoria) and one based in QCIF (Brisbane, Queensland). The clusters have seperate file systems, so you will have to transfer your files across if you'd like to switch regions.

## Monash

The Monash region is designed to be flexible. Users use CPU notebook or terminal sessions along with a Dask scheduler as an interface to the HPC system. This allows users to change the amount of compute they request are using entirely within the notebook and return valuable resources when they no longer need it. See [this FAQ](/faq.qmd#how-much-compute-should-i-ask-for-with-my-slurmcluster) for advice on requesting resources through the Dask scheduler.

Monash workloads are also able to make use of attached NVME for temporary file storage.

Notebook or terminal sessions in three sizes:  

- Small 
  - 4 VCPUs 
  - 16 GB RAM 
- Medium  
  - 8 VCPUs
  - 32 GB RAM  
- Large  
  - 8 VCPUs
  - 64 GB RAM  

Six nodes with:

- 54 VCPUs
- 460 GB RAM
- 1.5 TB NVME
- 2 A100s, each split with MIG into:
  - 1 20 GB VRAM
  - 2 10 GB VRAM

## QCIF
The QCIF region uses a more traditional approach with GPU enabled notebook or terminal sessions. This user experience is much closer to using a service like Google Colab.

GPU notebook or terminal sessions in three sizes:

- Regular
  - 20 VCPUs
  - 190 GB RAM
  - 10 GB VRAM
- Double
  - 20 VCPUs
  - 190 GB RAM
  - 20 GB VRAM
- Node
  - 60 VCPUs
  - 570 GB RAM
  - 2x 10 GB VRAM + 1x 20 GB VRAM
  