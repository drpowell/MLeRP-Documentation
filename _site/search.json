[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About MLeRP",
    "section": "",
    "text": "MLeRP is split into two clusters which users can choose between - one based in Monash (Melbourne, Victoria) and one based in QCIF (Brisbane, Queensland). The clusters have seperate file systems, so you will have to transfer your files across if you’d like to switch regions."
  },
  {
    "objectID": "about.html#compute",
    "href": "about.html#compute",
    "title": "About MLeRP",
    "section": "Compute",
    "text": "Compute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPartition\nNodes\nCPU Cores per node\nTotal CPU cores in partition\nMemory per node (GB)\nCPU Speed\nProcessor model\nCodename\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\nx"
  },
  {
    "objectID": "about.html#gpu-compute",
    "href": "about.html#gpu-compute",
    "title": "About MLeRP",
    "section": "GPU Compute",
    "text": "GPU Compute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPartition\nNodes\nCPU Cores per node\nTotal CPU cores in partition\nMemory per node (GB)\nNumber of GPUs per node\nTotal GPUs in partition\nGPU model\nGPU cores per card\nCPU Speed\nProcessor model\nCodename\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx"
  },
  {
    "objectID": "about.html#remote-desktops-through-the-cvl",
    "href": "about.html#remote-desktops-through-the-cvl",
    "title": "About MLeRP",
    "section": "Remote desktops through the CVL",
    "text": "Remote desktops through the CVL\nI assume we cut this?"
  },
  {
    "objectID": "about.html#compute-1",
    "href": "about.html#compute-1",
    "title": "About MLeRP",
    "section": "Compute",
    "text": "Compute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPartition\nNodes\nCPU Cores per node\nTotal CPU cores in partition\nMemory per node (GB)\nCPU Speed\nProcessor model\nCodename\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\nx"
  },
  {
    "objectID": "about.html#gpu-compute-1",
    "href": "about.html#gpu-compute-1",
    "title": "About MLeRP",
    "section": "GPU Compute",
    "text": "GPU Compute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPartition\nNodes\nCPU Cores per node\nTotal CPU cores in partition\nMemory per node (GB)\nNumber of GPUs per node\nTotal GPUs in partition\nGPU model\nGPU cores per card\nCPU Speed\nProcessor model\nCodename\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx"
  },
  {
    "objectID": "about.html#remote-desktops-through-the-cvl-1",
    "href": "about.html#remote-desktops-through-the-cvl-1",
    "title": "About MLeRP",
    "section": "Remote desktops through the CVL",
    "text": "Remote desktops through the CVL\nI assume we cut this?"
  },
  {
    "objectID": "dask-pytorch.html",
    "href": "dask-pytorch.html",
    "title": "MLeRP User Guide Documentation",
    "section": "",
    "text": "PyTorch and Dask\n\nDefining models, datasets and functions\nIf you’re doing something relatively simple, Dask has integrations with Scikit-Learn and XGBoost. You can also pass PyTorch models into Scikit-Learn with Skorch and TensorFlow models with SciKeras.\nBut if you need to do something more complex, Dask clusters can have python functions submitted to them to remotely execute code. This gives us the low level control to implement whatever bespoke algorithm we want and have it accelerated by remote GPUs.\nIn this example we’re going to write our own PyTorch functions to train a custom model on the CIFAR dataset. While we could do this with Skorch, we hope that this example gives you some idea of how Dask can be flexible enough for any applications that you need.\nContent adapted from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.multiprocessing as mp\n\n# Define data transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n# Define dataset and dataloader\nbatch_size = 1024\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\nvalidset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                        download=True, transform=transform)\n\n# Note that we need to set the multiprocessing context so that PyTorch doesn't get\n# PyTorch likes to use 'forking' while Dask uses 'spawn'\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=16, multiprocessing_context=mp.get_context(\"fork\"))\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n                                          shuffle=True, num_workers=16, multiprocessing_context=mp.get_context(\"fork\"))\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define a simple conv net\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n        self.conv6 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(4 * 4 * 64, 4 * 64)\n        self.fc2 = nn.Linear(4 * 64, 64)\n        self.fc3 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.relu(self.conv6(x))\n        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nThis train function will load any saved state for the provided model, then train for a number of epochs. When its done it will then save the state and return the average loss of the last epoch.\nThere are also a few flags which have been included to show the printing and error handling behaviour of dask clusters.\n\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm\n\n\n# loader: train dataloader\n# arch: model archetechture for training\n# path: model path for load and save\n# load: whether to load model from path\n# save: whether to save model to path\n# test: only run one batch for testing\n# error: throw an assertion error\n# return: average loss of epoch or loss of one batch if testing\ndef train(loader, arch=Net, path=\"./model\", epochs=1, load=False, save=True, test=False, error=False):\n    model = arch()\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Load state from disk so that we can split up the job\n    if load: \n        state = torch.load(path, map_location=\"cpu\")\n        model.load_state_dict(state[\"model\"])\n        model.to(device)\n        optimizer.load_state_dict(state[\"optimizer\"])\n    else:\n        model.to(device)\n    \n    # A typical PyTorch training loop\n    model.train()\n    for _ in range(epochs):\n        running_loss = 0\n        \n        for i, (inputs, labels) in enumerate(loader):\n            # put the inputs on the device\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.detach().item()\n            \n            # Force an error\n            if error:\n                assert 0 == 1\n            \n            # Stop after one batch when testing        \n            if test: \n                print(\"When running in a local cluster you can see print statements\")\n                break\n    \n    # Save model after each epoch\n    if save:\n        torch.save({\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict()\n            }, path)\n    \n    return running_loss / len(loader) if not test else loss.detach().item()\n\nThis valid function will load the state of the model we’ve defined, then calculate the average loss and accuracy over the dataset.\n\n# loader: train dataloader\n# arch: model archetechture for validating\n# path: model path for load and save\n# return: average loss and accuracy of epoch\ndef valid(loader, arch=Net, path=\"./model\"):\n    # Initialise device\n    model = arch()\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Load state from disk so that we can split up the job\n    state = torch.load(path, map_location=\"cpu\")\n    model.load_state_dict(state[\"model\"])\n    model.to(device)\n    model.eval()\n    \n    # A typical PyTorch validation loop\n    running_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(loader):\n            # put the inputs on the device\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # forward\n            outputs = model(inputs)\n            \n            # loss\n            loss = criterion(outputs, labels)\n            running_loss += loss.detach().item()\n            \n            # accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return running_loss / len(loader), correct / total\n\n\n\nTesting with a LocalCluster\nDask LocalClusters are easiest to use interactive development. This will mean that code will execute in the notebook session allowing you to view print statements and debug errors normally rather than dealing with remote code execusion before we’re ready. Later, when you are satisfied with your code you can switch over to a SLURMCluster to accelerate with GPU.\nDask prefers to control all processes so that it can manage them more gracefully if they fail, but we need to give PyTorch the control to use multiprocessing as needed. To do this set proccesses=False to allow for multiprocessing inside Dask jobs.\n\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster(processes=False)\nclient = Client(cluster)\nclient\n\n/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/node.py:179: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 41665 instead\n  warnings.warn(\n\n\n\n\n     \n    \n        Client\n        Client-a5aae8d9-bbdd-11ed-a7fd-fa163e16ef68\n        \n\n        \n        \n            Connection method: Cluster object\n            Cluster type: distributed.LocalCluster\n        \n        \n\n        \n            \n                \n                    Dashboard:  http://192.168.0.213:41665/status\n                \n                \n            \n        \n\n        \n\n        \n            \n            Cluster Info\n            \n    \n    \n    \n        LocalCluster\n        daf492d3\n        \n            \n                \n                    Dashboard: http://192.168.0.213:41665/status\n                \n                \n                    Workers: 1\n                \n            \n            \n                \n                    Total threads: 20\n                \n                \n                    Total memory: 590.14 GiB\n                \n            \n            \n            \n    Status: running\n    Using processes: False\n\n\n            \n        \n\n        \n            \n                Scheduler Info\n            \n\n            \n    \n         \n        \n            Scheduler\n            Scheduler-ba375271-3eec-4370-923c-27c43ee917a0\n            \n                \n                    \n                        Comm: inproc://192.168.0.213/387069/1\n                    \n                    \n                        Workers: 1\n                    \n                \n                \n                    \n                        Dashboard: http://192.168.0.213:41665/status\n                    \n                    \n                        Total threads: 20\n                    \n                \n                \n                    \n                        Started: Just now\n                    \n                    \n                        Total memory: 590.14 GiB\n                    \n                \n            \n        \n    \n\n    \n        \n            Workers\n        \n\n        \n        \n             \n            \n            \n                \n                    Worker: 0\n                \n                \n                    \n                        \n                            Comm:  inproc://192.168.0.213/387069/4\n                        \n                        \n                            Total threads:  20\n                        \n                    \n                    \n                        \n                            Dashboard:  http://192.168.0.213:36253/status\n                        \n                        \n                            Memory:  590.14 GiB\n                        \n                    \n                    \n                        \n                            Nanny:  None\n                        \n                        \n                    \n                    \n                        \n                            Local directory:  /tmp/dask-worker-space/worker-rhvj9n9u\n                        \n                    \n\n                    \n\n                    \n\n                \n            \n            \n        \n        \n\n    \n\n\n        \n    \n\n            \n        \n\n    \n\n\n\nWe can submit our function to the cluster with the client.submit method. This will return a future which can be unpacked with its result using future.result(). We can see the outputs of print statements while we’re using a LocalCluster. Print statements will not be visible when executing remotely with SLURMCluster.\n\nfuture = client.submit(train, trainloader, test=True)\nfuture.result()\n\n/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/worker.py:2845: UserWarning: Large object of size 146.58 MiB detected in task graph: \n  [<torch.utils.data.dataloader.DataLoader object at 0x7fa2783c7af0>]\nConsider scattering large objects ahead of time\nwith client.scatter to reduce scheduler burden and \nkeep data on workers\n\n    future = client.submit(func, big_data)    # bad\n\n    big_future = client.scatter(big_data)     # good\n    future = client.submit(func, big_future)  # good\n  warnings.warn(\n\n\nWhen running in a local cluster you can see print statements\n\n\n2.3040761947631836\n\n\nWe can use the client.scatter method to scatter large objects out to our workers ahead of time for more efficient execution.\n\ntrainloader_future = client.scatter(trainloader)\nclient.submit(train, trainloader, test=True).result()\n\nWhen running in a local cluster you can see print statements\n\n\n2.3033735752105713\n\n\n\nclient.shutdown()\n\n\n\nTraining with a SLURMCluster\nDask usually uses a ‘nanny’ that monitors any worker processes and gracefully restarts them if they fail or are killed while performing computations. The nanny is not compatable with daemonic processes - that is dask workers cannot perform multiprocessing while it’s being used. We therefore need to set nanny=False to turn off the nanny to allow for multiprocessing inside Dask jobs for the cluster to work with PyTorch. (Just like when we processes=False for the LocalCluster.)\nWe can pass in extra SLURM requirements in job_extra_directives to request a GPU for our jobs. To read more about configuring the SLURMCluster to interact with the SLURM queue, go to Dask’s jobqueue documentation.\n\nfrom dask_jobqueue import SLURMCluster\nfrom distributed import Client\ncluster = SLURMCluster(\n    memory=\"190g\", processes=1, cores=20, job_extra_directives=[\"--gres=gpu:1\"], nanny=False\n)\n\ncluster.scale(1)\nclient = Client(cluster)\n\n/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/node.py:179: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 34527 instead\n  warnings.warn(\n\n\nSince this code is executing remotely we won’t see our print statements\n\ntrainloader_future = client.scatter(trainloader)\nclient.submit(train, trainloader_future, test=True).result()\n\n2.3033061027526855\n\n\nDask will raise any errors that the process triggers locally, even when executing remotely\n\ntrainloader_future = client.scatter(trainloader)\nclient.submit(train, trainloader_future, error=True).result()\n\nAssertionError: \n\n\nFinally we can bring everything together and run our training loop.\n\n# Run the training loop\nepochs = 5\ntrainloader_future = client.scatter(trainloader)\nvalidloader_future = client.scatter(validloader)\nwith tqdm(total=(epochs)) as pbar:\n    for epoch in range(epochs):\n        train_loss = client.submit(train, trainloader_future, load=(epoch > 0)).result()\n        valid_loss, accuracy = client.submit(valid, validloader_future).result()\n        pbar.update()\n        pbar.set_postfix(loss=train_loss)\n        print( f\"epoch: {epoch}, train_loss: {train_loss : .3f}, valid_loss: {valid_loss : .3f}, accuracy: {accuracy : .3f}\")\n\n\n\n\nepoch: 0, train_loss:  2.271, valid_loss:  2.142, accuracy:  0.206\nepoch: 1, train_loss:  2.019, valid_loss:  1.934, accuracy:  0.284\nepoch: 2, train_loss:  1.897, valid_loss:  1.834, accuracy:  0.322\nepoch: 3, train_loss:  1.816, valid_loss:  1.766, accuracy:  0.344\nepoch: 4, train_loss:  1.750, valid_loss:  1.700, accuracy:  0.368\n\n\n\n\nMeasuring Dask’s overhead\nOffloading tasks to Dask doesn’t come for free, there is an initial cost associated with sending the data to a remote device. Let’s compare the time it would take to train a Resnet18 on CIFAR for a range of epochs comparing a local GPU, a remote GPU using Dask and a remote GPU using Dask with a scattered dataset. For this expriment we will not bother saving the weights afterwards since this should be relatively constant between methods.\n\nfrom torchvision.models import resnet18\nfrom time import time\n\n# Store times in arrays\nlocal = []\nremote = []\nscatter = []\n\n# Test some number of epochs\nepoch_list = [1, 2, 3, 5, 10]\nwith tqdm(total=(len(epoch_list) * 3)) as pbar:\n    for num_epochs in epoch_list:\n        \n        # Local GPU\n        start = time()\n        train(trainloader, arch=resnet18, epochs=(num_epochs + 1), save=False)\n        local.append(time() - start)\n        pbar.update()\n        \n        # Remote GPU\n        start = time()\n        client.submit(train, trainloader, arch=resnet18, epochs=(num_epochs + 1), save=False).result()\n        remote.append(time() - start)\n        pbar.update()\n        \n        # Remote GPU with scatter\n        start = time()\n        trainloader_future = client.scatter(trainloader)\n        client.submit(train, trainloader_future, arch=resnet18, epochs=(num_epochs + 1), save=False).result()\n        scatter.append(time() - start)\n        pbar.update()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\ndata = list(chain(*zip(local, remote, scatter)))\ncolumns = []\nfor num_epochs in epoch_list:\n    for test in [\"local\", \"remote\", \"scatter\"]:\n        columns.append(test + \" \" + str(num_epochs))\n\nplt.bar(range(len(data)), data, tick_label=columns)\nplt.xticks(rotation=90)\nplt.xlabel(\"Experiment\")\nplt.ylabel(\"Seconds\")\nplt.title(\"Runtime comparison for local, remote and scatter\")\nplt.show()\n\n\n\n\nFrom this experiment we can see that the cost associated with running code remotely is small, and the impact decreases with the size of the function that we submit. It also shows that it always makes sense to scatter large objects before computing, even for small jobs.\n\nclient.shutdown()"
  },
  {
    "objectID": "dask-slurm.html",
    "href": "dask-slurm.html",
    "title": "MLeRP User Guide Documentation",
    "section": "",
    "text": "Dask SLURMClusters\nThe MLeRP notebook environment uses Dask SLURMClusters to create a middle ground that has the interactivity of a notebook backed by the power of HPC. You will be provisioned with a CPU based notebook session for your basic analysis and code development. Then, when you’re ready to run testsm you will use Dask to submit your python functions to the SLURM queue.\nThis enables: - Flexibility to experiment with your dataset interactively - Ability to change compute requirements such as RAM, size of GPU, number of processes and so on… without ever leaving the notebook environment - Elastic scaling of compute - Efficient utilisation of the hardware - Releasing of resources when not in use\n\nfrom dask_jobqueue import SLURMCluster\nfrom distributed import Client, LocalCluster\nimport dask\n\n# Point Dask to the SLURM to use as it's back end\ncluster = SLURMCluster(\n    memory=\"64g\", processes=1, cores=8\n)\n\n# Scale out to 4 nodes\nnum_nodes = 4\ncluster.scale(num_nodes)\nclient = Client(cluster)\nclient\n\n/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/node.py:179: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 46719 instead\n  warnings.warn(\n\n\n\n\n     \n    \n        Client\n        Client-fc123988-c131-11ed-818a-fa163e16ef68\n        \n\n        \n        \n            Connection method: Cluster object\n            Cluster type: dask_jobqueue.SLURMCluster\n        \n        \n\n        \n            \n                \n                    Dashboard:  http://192.168.0.213:46719/status\n                \n                \n            \n        \n\n        \n\n        \n            \n            Cluster Info\n            \n    \n    \n    \n        SLURMCluster\n        e275099f\n        \n            \n                \n                    Dashboard: http://192.168.0.213:46719/status\n                \n                \n                    Workers: 0\n                \n            \n            \n                \n                    Total threads: 0\n                \n                \n                    Total memory: 0 B\n                \n            \n            \n        \n\n        \n            \n                Scheduler Info\n            \n\n            \n    \n         \n        \n            Scheduler\n            Scheduler-257a694b-35cf-4fdb-b907-b030d5390c7a\n            \n                \n                    \n                        Comm: tcp://192.168.0.213:40885\n                    \n                    \n                        Workers: 0\n                    \n                \n                \n                    \n                        Dashboard: http://192.168.0.213:46719/status\n                    \n                    \n                        Total threads: 0\n                    \n                \n                \n                    \n                        Started: Just now\n                    \n                    \n                        Total memory: 0 B\n                    \n                \n            \n        \n    \n\n    \n        \n            Workers\n        \n\n        \n\n    \n\n\n        \n    \n\n            \n        \n\n    \n\n\n\nDask will now spin our jobs up in anticipation for work to the scale that you specify.\nYou can check in on your jobs like you would with any other SLURM job with squeue.\n\n!squeue\n\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n              1637     batch Jupyter  mhar0048  R      42:23      1 mlerp-node05\n              1638     batch dask-wor mhar0048  R       0:16      1 mlerp-node05\n              1639     batch dask-wor mhar0048  R       0:16      1 mlerp-node05\n              1640     batch dask-wor mhar0048  R       0:16      1 mlerp-node05\n              1641     batch dask-wor mhar0048  R       0:16      1 mlerp-node05\n\n\nAlternatively, we can use the adapt method, which will let us scale out as we need the compute… and scale back when we’re idle letting others use the cluster.\nWe reccommend that you use the adapt method while you’re actively developing your code so that you don’t need to worry about cleaning up after yourself. The scale method can be used when you’re ready to run longer tests with higher utilisation.\n\ncluster.adapt(minimum=0, maximum=num_nodes)\n\n<distributed.deploy.adaptive.Adaptive at 0x7f85ea9ec820>\n\n\n\n# You may need to run this cell a few times while waiting for Dask to clean up\n!squeue\n\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n              1637     batch Jupyter  mhar0048  R      43:01      1 mlerp-node05\n\n\nDask has a UI that will let you see how the tasks are being computed. You won’t be able to connect to this with your web browser but VSCode and Jupyter have extensions for you to connect to it.\nUse the loopback address: http://127.0.0.1:8787 (Adjust the port to the one listed when you make the client if needed)\nNow let’s define a dask array and perform some computation. Dask arrays are parallelised across your workers nodes so they can be greater than the size of one worker’s memory. Dask evaluates lazily, retuning ‘futures’ which record the tasks needed to be completed in the compute graph. They can be computed later for its value.\nDask also has parallelised implementations of dataframes and collections of objects (called bags). These are written to be as similar as possible to familiar libraries like numpy, pandas and pyspark. You can read more about arrays, dataframes and bags with Dask’s documentation.\n\nimport dask.array as da\nx = da.random.random((1000, 1000, 1000))\nx  # Note how the value of the array hasn't been computed yet\n\n\n\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         7.45 GiB \n                         119.21 MiB \n                    \n                    \n                    \n                         Shape \n                         (1000, 1000, 1000) \n                         (250, 250, 250) \n                    \n                    \n                         Count \n                         64 Tasks \n                         64 Chunks \n                    \n                    \n                     Type \n                     float64 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  1000\n  1000\n  1000\n\n        \n    \n\n\n\nYou can check squeue while this is running to see the jobs dynamically spinning up to perform the computation.\n\nx[0][0][:10].compute()\n\narray([0.9527929 , 0.93675059, 0.11717679, 0.47114357, 0.73693508,\n       0.01302143, 0.86360879, 0.12592881, 0.52676823, 0.99186392])\n\n\nWe can also accelerate dask arrays with GPUs using cupy. There is similar support for accelerating dask dataframes with CuDF.\n\ndask.config.set({\"array.backend\": \"cupy\"})\ny = da.random.random((1000, 1000, 1000))\ny.compute()\ny[0][0][:10].compute()\n\narray([0.02380941, 0.62371184, 0.88393467, 0.8604588 , 0.16488854,\n       0.11214214, 0.86582312, 0.01384666, 0.79636323, 0.58940477])\n\n\nFinally, we can shut down the SLURMCluster now that we’re done with it.\n\n# Shut down the cluster\nclient.shutdown()"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "MLeRP FAQ",
    "section": "",
    "text": "Say we have a researcher who is a domain expert in a field and is just discovering that machine learning algorithms might be appropriate to accelerate their work. They explore online for potential approaches and even find some tutorials that use interactive environments, but quickly discover that dealing with large datasets requires more compute than their laptop can handle so they need to move to a HPC system. While desktop sessions are available they might have prohibitively long queues and have low utilisation of the underlying hardware. They could submit a job to the HPC queue but this leads to long times between iterations. At this stage the researcher is still discovering things about their data, how to clean it and how to analyse it.\nThey need an interactive environment so they can develop and debug their algorithms and the algorithms, but still need access to high powered acceleration so they can process their dataset. At the same time we want to improve utilisation of the hardware so that we can serve more users and reduce wait times.\nMLeRP was built to be a middle ground that has the interactivity of a notebook with the power of a HPC environment that can share valuable resources between other users while code isn’t being executed.\nNow our new researcher can easily import the same code that they were using on their laptops or with online notebook services straight into a HPC environment without the need to convert it first into a SBATCH script, wait in long HPC queues or load modules for dependencies."
  },
  {
    "objectID": "faq.html#why-do-i-still-have-to-wait-in-queue",
    "href": "faq.html#why-do-i-still-have-to-wait-in-queue",
    "title": "MLeRP FAQ",
    "section": "Why do I still have to wait in queue?",
    "text": "Why do I still have to wait in queue?\nDirectly attaching GPU compute to notebooks leads to very low utilisation since the GPU sits idle while you’re debugging your code. By using a queue we are able to service more researchers at once with greater efficiency."
  },
  {
    "objectID": "faq.html#if-i-still-have-to-wait-in-queue-how-is-this-better-than-using-a-traditional-hpc-environment",
    "href": "faq.html#if-i-still-have-to-wait-in-queue-how-is-this-better-than-using-a-traditional-hpc-environment",
    "title": "MLeRP FAQ",
    "section": "If I still have to wait in queue how is this better than using a traditional HPC environment?",
    "text": "If I still have to wait in queue how is this better than using a traditional HPC environment?\nMLeRP is designed with the idea that a job’s size should be about the size of a cell in a jupyter notebook. With that in mind, the queue has been optimised for short jobs, with a wall time of |NEED TO WORK OUT|. Executing a cell won’t always be immediate but your job should start up pretty quickly. If you need to run a job that runs longer than this, checkpoint your code and split it into multiple jobs. If your job takes longer than |NEED TO WORK OUT|, contact us at |EMAIL HERE|"
  },
  {
    "objectID": "faq.html#why-dask",
    "href": "faq.html#why-dask",
    "title": "MLeRP FAQ",
    "section": "Why Dask?",
    "text": "Why Dask?\nWe looked at a few different options for the MLeRP environment including Sagemaker, Spark and Ray. Ultimately we settled on Dask as the primary tool to interface with the queue because:\n\nSLURM jobs can be submitted with SLURMCluster whenever HPC is needed\nDebugging can be done locally first with LocalCluster with minimal code change\nIt has a familiar syntax as it’s designed as a light wrapper around common libraries like Numpy, Pandas and SciKit-Learn\nHigh level applications can be implemented easily with Scikit-Learn, XGBoost, Skorch, SciKeras\nDask can submit any python function to the SLURM queue allowing the flexibility for bespoke low level applications\nLazy evaluation of functions which allows for asynchronous code"
  },
  {
    "objectID": "faq.html#will-i-need-to-change-my-code-to-work-with-mlerp",
    "href": "faq.html#will-i-need-to-change-my-code-to-work-with-mlerp",
    "title": "MLeRP FAQ",
    "section": "Will I need to change my code to work with MLeRP?",
    "text": "Will I need to change my code to work with MLeRP?\nYes. Dask unfortunately does not come for free, you will have to do some code change to use it to interface with the cluster. You will also need to get a sense of SLURM ’s parameters defaults but it is\nThat said we believe that this approach of submitting jobs through a python notebook environment will feel more familiar to researchers familiar with the python datascience ecosystem given how Dask is designed as a light wrapper around common libraries like numpy, pandas and Scikit-Learn.\nYou will also be able to work with the cluster without needing to convert your experimental notebook code into a script and maintain the environment with modules like with a traditional cluster."
  },
  {
    "objectID": "faq.html#why-arent-my-print-statements-showing-up-in-my-jobs",
    "href": "faq.html#why-arent-my-print-statements-showing-up-in-my-jobs",
    "title": "MLeRP FAQ",
    "section": "Why aren’t my print statements showing up in my jobs?",
    "text": "Why aren’t my print statements showing up in my jobs?\nPrint statements that are executed on remote machines won’t show up in your notebook. If you are using print statements for debugging, consider using a LocalCluster where they will behave as expected.\nIf you need to record information while code is executing remotely either pass the information back to the notebook when the function returns for it to be printed, or log the output to a file."
  },
  {
    "objectID": "faq.html#how-much-compute-should-i-ask-for-with-my-slurmcluster",
    "href": "faq.html#how-much-compute-should-i-ask-for-with-my-slurmcluster",
    "title": "MLeRP FAQ",
    "section": "How much compute should I ask for with my SLURMCluster?",
    "text": "How much compute should I ask for with my SLURMCluster?\nUnfortunately there is no one size fits all answer to this question. Every research problem has its on demands and constraints so this a bit like asking ‘How long is a piece of string?’.\nWith that being said, our GPU compute is split into |FLAVOURS| sizes and they are on nodes with |THIS MUCH RAM|. As a default it may make sense to start with a fraction of RAM that is proportional to your chosen GPU size.\nIf your work is primarily CPU bound, Dask allows as many tasks as you have CPU cores, but if you are using GPUs you may want to dramatically limit this. One process per GPU could be a good place to start.\nFor more information about using SLURMCluster, visit Dask’s documentation"
  },
  {
    "objectID": "faq.html#should-i-use-cluster.scale-or-cluster.adapt",
    "href": "faq.html#should-i-use-cluster.scale-or-cluster.adapt",
    "title": "MLeRP FAQ",
    "section": "Should I use cluster.scale or cluster.adapt?",
    "text": "Should I use cluster.scale or cluster.adapt?\nWe reccommend that you use the adapt method while you’re actively developing your code so that you don’t need to worry about cleaning up after yourself. The scale method can be used when you’re ready to run longer tests with higher utilisation."
  },
  {
    "objectID": "faq.html#how-do-i-install-my-favourite-python-package",
    "href": "faq.html#how-do-i-install-my-favourite-python-package",
    "title": "MLeRP FAQ",
    "section": "How do I install my favourite python package?",
    "text": "How do I install my favourite python package?\nIf you want to control the python environment we recommend that you install and maintain a miniconda environment in your userdata directory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the MLeRP User guide",
    "section": "",
    "text": "The MLeRP environment creates a middle ground that has the interactivity of a notebook with the power of a HPC environment that can share valuable resources between other users while code isn’t being executed.\nWe provide users with CPU based Jupyter notebook sessions capable of basic analysis, with the ability to interactively send jobs to a SLURM queue for GPU or parallelised CPU acceleration through Dask.\nBe sure to check out the Jupyter, SLURM and Dask documentation for more information.\nMLeRP at the moment is only available through closed beta but is planned to be available through open beta to all Australian researchers. You can sign up to our mailing list if you’d like us to let you know when we’re ready for open beta.\n\n\nLog in\n\n\nSign up\n\n\n\n\nContact us\nWant to get in touch with the developers? You can reach us at: EMAIL HERE"
  },
  {
    "objectID": "strudelv2_spa/docs/applications.html",
    "href": "strudelv2_spa/docs/applications.html",
    "title": "MLeRP User Guide Documentation",
    "section": "",
    "text": "You need to define a program to run and put the value in startscript. You can’t use #SBATCH pragmas here though. You also need to define a program that will take a jobid and return the port the program is running on and anything else (like access tokens or passwords). This is a paramscmd. The paramscmd can return a json error message. The paramscmd might look easy and a proof of concept might take 5 minutes, but then you get into edge cases and it turns out to be the hardest bit so try to copy an existing one. Next you need the URL to connect to (eg index.html?token=adsf). This is a realtive URL (i.e. if you would normally use ssh tunnels and localhost: drop the localhost part Finally you put all this data into a json structure and save it to the config files.\n\n\n\nStrudel applications look like this\n {\n   \"url\": null,\n   \"name\": \"Jupyter Lab\",\n   \"startscript\": \"#!/bin/bash\\n/usr/local/sv2/dev/jupyter/jupyter.slurm\\n\",\n   \"actions\": [ \n       {\n           \"name\": \"Connect\",\n           \"paramscmd\": \"/usr/local/sv2/dev/jupyter/jupyter_params.py {jobid}\",\n           \"client\": {\"cmd\": null, \"redir\": \"?token={token}\"},\n           \"states\": [\"RUNNING\"]\n       },\n       {\n           \"name\": \"View log\",\n           \"paramscmd\": \"/usr/local/sv2/dev/desktop/logparams.py {jobid}\",\n           \"client\": {\"cmd\": null, \"redir\": \"index.html?token={token}\" },\n           \"states\": [\"RUNNING\",\"Finished\"]\n       },\n        {\n            \"name\": \"View Usage\",\n            \"paramscmd\": \"/usr/local/sv2/dev/desktop/usageparams.py {jobid}\",\n            \"client\": {\"cmd\": null, \"redir\": \"index.html?token={token}\" },\n            \"states\": [\"Finished\"]\n        },\n       {\n           \"name\": \"Remove log\",\n           \"paramscmd\": \"/usr/local/sv2/dev/rmlog.py {jobid}\",\n           \"client\": null,\n           \"states\": [\"Finished\"]\n       }\n\n   ],\n   \"localbind\": true,\n   \"applist\": null\n  },\nThis is block of json data. It gets stored in a config file. Each compute site has its own list of applications. And if you are running dev and test environments you probably have a different set of applications on each. For M3 the applications are deployed as part of the frontend build (because it was easy to combine the config and the code) but for other sites the URL for this configuration data might be completely independent. For M3 the applications deployed to dev are defined here\nhttps://gitlab.erc.monash.edu.au/hpc-team/strudelv2_spa/blob/dev/src/assets/config/m3apps.dev.json\nand the applications for test are here\nhttps://gitlab.erc.monash.edu.au/hpc-team/strudelv2_spa/blob/dev/src/assets/config/m3apps.test.json\nIn order to deploy a new application you sould edit those files on dev and create a merge request\n\n\n\nThe first value url allows up to specify a URL that will provide additional configuration info. We don’t use this for desktops or Jupyter but we might use it for transfering files. The URL should open in an iframe and use window.message methods to pass data back to Strudel2. For most applications this will be set to None.\nthe name value is reasonably self explinatory but its worth noting: When the form is rendered for what resources to use (CPUs/GPUs/Time) this value is passed to the form so that for example the application named “Desktop” renders a different form than the applictaion named “Jupyter Lab”. Don’t the M3 forms will render a good default for any unknown applicaiton names, so you can pretty much fill in whatever you like.\nThe startscript gets passed as stdin to whatever command the site runs things with. In the case of M3 this is sbatch, so this contents (start script) gets passed to sbatch. In this example /usr/local/sv2/dev/jupyter/jupyter.slurm i NOT a slurm script. i.e. if you put #SBATCH lines in there they will be ignored. It is a program. If you need to do #SBATCH lines it should be like \"startscript\": \"#!/bin/bash\\n#SBATCH -w m3a011\\n/usr/local/sv2/dev/jupyter/jupyter.slurm\\n\", but you really shouldn’t do this. The intention is that the start script doesn’t care what job scheduler your using\nNext we have a list of actions. Each of these renders a button in Strudel2 depending on the state of the Job. For each action what happens is tha the the paramscmd gets run and returns a blob of json data. Then the client is executed using the data returned from the paramscmd. The paramscmd should always return a value for port i.e. the network port to connect on and should also return any info used in the client definition (so jupyter_parms.py must return both a port and a token like {\"port\":123,\"token\":\"abc\"}) You sould attempt to copy your params cmd off an existing implementation. Ideally they are not aware of what batch environment is used so that a paramscmd used on a PBS site can be shared with a slurm site. In practice we’ve had to make our paramcmds aware of the jobid and the process tree origining from that job id in order to connect to the correct job (incase multiple jobs are running on the same node)\nYou’ll notice tha tthe client defines both a cmd and a redir. The cmd field is reserved for future work where Strudle2 can be installed locally and use things like a native vnc viewer instead of a web browser and noVNC.\nNext we have the localbind option. This should be set to true. It controls the behaviour of tunnels. In particular you generally can’t access Jupyter from the login node, you have to ssh to the execution host and access using ssh -L 8888:localhost:8888 <exechost> On the other hand if you want to access the SSH server on the execution host you don’t need to do ssh -L 2222:localhost:22 <exechost> you can get stright there from the login node.\nFinally the applist option allows for recursivly nexting another list of apps. I implement this feature in the S2 UI, but because its not currently in use and has no test coverage its probably got some bit rot. If you feel the need to have a multilevel list of applications, please contact the developer."
  },
  {
    "objectID": "strudelv2_spa/docs/conda.html",
    "href": "strudelv2_spa/docs/conda.html",
    "title": "MLeRP User Guide Documentation",
    "section": "",
    "text": "Conda on M3\nconda (anaconda and miniconda) are great ways to create a python environment. Unfortunately for some reason they edit your .bashrc file creating a situation where MASSIVE desktops no longer work. We provide a wrapper script to help create conda environments without this unforunate side effect\n\n\nCreating a Conda environment\nPlease open a connection to M3 (you might use the strudel2 terminal to do this, or a native terminal) and follow these steps\n\n$ cd <minicondapath>\n\n# Install miniconda\n\n$ module load conda-install\n$ conda-install <minicondapath>\n\nreplacing <minicondapath> with an actual directory. By default we set <mincondapath> to /scratch/<projectid>/<username>/miniconda\n\n\nUsing the default JupyterLab environement\nThe conda-install script (by default) sets up a jupyter environment and makes it available by strudel2. You can use this immediately by selecting the approperiate option on strudel2\nIf you want to add additional packages to the jupyter lab environment\nsource <minicondapath>/bin/activate\nconda activate jupyterlab\nconda install <packagename>\n\n\nCreating other (non-Jupyter-non-Strudel2) environments\nsource <minicondapath>/bin/activate\nconda create <environmentname>\nconda activate <environmentname>\n\n\nStopping conda\n(jupyterlab) [chines@m3-login2 ~]$ conda deactivate\n(base) [chines@m3-login2 ~]$ conda deactivate\nNote that the first deactivate, deactivates the current environement. The second deactivate deactivates conda entirely (at which point you will have to return the the instruction to source <minicondapath>/bin/activate)"
  },
  {
    "objectID": "strudelv2_spa/docs/strudel2.html",
    "href": "strudelv2_spa/docs/strudel2.html",
    "title": "MLeRP User Guide Documentation",
    "section": "",
    "text": "Frontend static files (html and .js) are fetched. This could be from a CDN but right now its on the same instance as the backend, with a different domain name\nThe Frontend generates and ssh key and gets a cert from SSHAuthZ. This is an OAuth2 Implicit flow (original sshauthz by Jason doesn’t support this so I wrote a new one). Access to the cert is mediated by OpenID Connect through either AAF or Google (if Apple gets their act together I’ll be happy to suport login with Apple)\nHaving got a certificate the frontend will be able to login to a number of clusters (remember MonARCH CVL@UWA and M3 share the same certificates). Config files will tell the frontend about the login nodes. The frontend (optionally) uses the key/cert to get user info (like quotas) and check cachet.\n\nAt the point the user will select an application (eg a desktop or a jupyter notebook) and the frontend will contact pull in an iframe loaded with the form for the particular login node. This form is reponsible for providing the sbatch command or the pbs command (each cluster therefor has a different form)\nHaving got the sbatch command the frontend can submit a job to the cluster\nAt this point all the communication is through the backend component TES (tunnel and execution service). TES understands simple GETs and PUTs as well as ssh tunnels.\n\nOnce the job is running, the user can click “Connect” This will open a new tab to to the url “strudel2-api.cloud.cvl.org.au”. This URL path connects to the TWS (Transparent WebSockets Proxy) component. This is cohosted in the same docker container as the TES. Unlike the TES which is written at the HTTP application layer, the TWS is written at the unix socket layer. The TWS will search incomming bytes for a magic cookie then connect these incomming bytes to the correct SSH tunnel."
  },
  {
    "objectID": "strudelv2_spa/docs/strudel2.html#components",
    "href": "strudelv2_spa/docs/strudel2.html#components",
    "title": "MLeRP User Guide Documentation",
    "section": "Components",
    "text": "Components\nEach component containts its own documentation directory for installation instructions.\nThis is the Strudel2 Frontend (an SPA written in angular).\nhttps://gitlab.erc.monash.edu.au/hpc-team/pysshauthz is the SSHAuthZ component needed go map web based login (i.e. OpenID Connect) to SSH certificates for access to the cluster\nhttps://gitlab.erc.monash.edu.au/hpc-team/batchbuilder is the code that provides a form for each application. This version is used by Monash and is specific to slurm."
  },
  {
    "objectID": "strudelv2_spa/docs/strudel2.html#configuration",
    "href": "strudelv2_spa/docs/strudel2.html#configuration",
    "title": "MLeRP User Guide Documentation",
    "section": "Configuration",
    "text": "Configuration\nThere are a number of configuration files for Strudel2. All configs and be subplemented by loading a local json file into the frontend.\n\nThe list of backends. Usually code served from strudel2-test connects to strudel2-api-test but this is not necessarily the case. The list of backends allows the user to choose the closest backend (in terms of network latency) or fail over from one backend to another in the even of an outage.\nThe list of SSH AuthZ servers. Each compute site is expected to operate the own sshauthz server, since if you control the CA you can impersonate any user at the site.\nThe list of Compute resources (i.e. clusters/sites). Each site\n\na login node\na CA which can issue certs for this login node\nwebsite capable fo generating the correct sbatch/qsub/bash command to execute programs\na command to list all jobs\na command to cancel a job\na list of apps\n\nThe list of apps. Each application defines:\n\na command to start the application\na number of actions to be preformed on the application (eg connecting to the application). Each action defines\n\nA command to return a json blob\nA URL that when filled in with data from the json blob will connect the user. Application lists have some extra parameters that are unused. For example applist allows you to define applications recursively (if you want to have desktops served via guacamole or novnc or xrdp). The can also define a URL for configuring the application (As opposed to the URL for configuring the cluster resources). That URL might allow you to configure input files for example. Some possible actions include: Connecting to the App, Viewing the output logs, viewing the sacct reporting for the app.\n\n\n\nCluster Scripts —————\nDepending on what applications a cluster support they will need some simple scripts to start the applications and generate json output (like what port is the application running on and what is its API token/password. For M3 these are installed in /usr/local/sv2 and /usr/local/sv2/dev (for development scripts). I was hoping that these would be independet of the cluster/job scheduler and could be installed into a container along with the application itself, but this is turning out not to be the case. For example, a user might run more than one jupyter notebook on a node, in which case we need a way to determin which notebook they were trying to connect to. The only way to do this turns out to be to use sstat to figure out which processes belong to which job."
  },
  {
    "objectID": "strudelv2_spa/docs/vscode.html",
    "href": "strudelv2_spa/docs/vscode.html",
    "title": "MLeRP User Guide Documentation",
    "section": "",
    "text": "[[TOC]]\nSo you’d like to use Visual Studio Code, but you need access to M3 too - maybe your data is on the cluster, or you want to run code interactively on a GPU, or you just really love VS Code. No problem. You want a VS Code remote server.\nPlease do NOT follow the official VS Code documentation for remote servers.\nBy default, if you follow those instructions, you will end up running the VS Code remote server on the login node. Please don’t do this, the login nodes are a shared resource and we would rather allocate a resource just for you."
  },
  {
    "objectID": "strudelv2_spa/docs/vscode.html#set-up-instructions-only-do-these-once",
    "href": "strudelv2_spa/docs/vscode.html#set-up-instructions-only-do-these-once",
    "title": "MLeRP User Guide Documentation",
    "section": "Set-Up Instructions (only do these once)",
    "text": "Set-Up Instructions (only do these once)\n\n1. Prepare VS Code\n\nInstall Visual Studio Code\nInstall the Remote Development Extension Pack\nOpen Settings with Ctrl+, and search for Remote.SSH: Remote Server Listen On Socket. Select the checkbox. \n\n\n\n2. Generate an ssh key and add it to the ssh agent\nYou may be familiar with using passwords to authenticate to M3 - ssh keys are another, more secure method of doing this. If you’re unfamiliar with ssh keys, no worries - we’ll step you through how to set one up here. If you want to learn more about why ssh keys are more secure than passwords, This YouTube video does a good job of explaining it, even if it is designed for an AWS user.\nThe process for creating keys and adding them to your ssh agent differs between Windows and Linux/Mac.\n\n\nMac and Linux Instructions\n\n\nNote: These instructions will refer to your local terminal, as well as the Strudel2 terminal. Pay attention!\nFirstly, we need to generate an ssh key-pair. Open a local terminal, and run the following command:\nssh-keygen -t ed25519\nYou will be asked where you would like to save the keys. Press enter to keep the default location. You will also be asked to enter a passphrase. This will be used to encrypt the private key on your computer. There will be a print out of your key’s fingerprint and location on the screen.\nYou should now have a public and private key in your ~/.ssh directory, which will be named id_ed25519.pub and id_ed25519 respectively. Never share your private key with someone else.\nSecondly, you will need to copy your public key into M3. To do this, run the following command:\ncat ~/.ssh/id_ed25519.pub                                                                                                       \nCopy the line of text printed on the screen.\nStart a terminal session in Strudel2 by selecting Terminal from the sidebar on the left-hand side of the screen, and then selecting Run on Login Node. This will open a terminal on M3 in a new tab. Ensure you have copied the public key from before, and run the following command in the Strudel2 terminal:\necho <paste the line you just copied> >> .ssh/authorized_keys\nThis will add your public key to M3 and allow you to authenticate using your new ssh-keys. Close the terminal tab and terminate the terminal session.\nFinally, you will want to add your ssh keys to the ssh-agent. You will remember setting a passphrase earlier for accessing your private key. When you add a private key to the ssh-agent, it will ask for your passphrase once, and then remember it until the next time your computer reboots. This means you won’t need to enter a passphrase everytime you log in to M3 with your keys.\nTo do this, open a local terminal and run:\nssh-add ~/.ssh/id_ed25519\n\n\n\nWindows Instructions\n\n\n\nInstall and enable OpenSSH\nBefore we can generate keys on Windows, OpenSSH has to be installed and enabled.\n\nCheck OpenSSH is installed.\n\n\nNavigate to Setting > Apps > Apps & Features\nSelect Optional Features.\nCheck if OpenSSH Client is in the list. If it is, go to step 2.\nIf it isn’t, select the Add a feature button at the top of the page, then select OpenSSH Client, and Install. This will install the OpenSSH Client.\n\n\nOnce installed, you will need to start the OpenSSH Client.\n\n\nUse the Windows search bar to search for and open Services.\nScroll through the list to find OpenSSH Authentication Agent\nDouble click OpenSSH Authentication Agent. Change the Startup type to Automatic and select OK.\nDouble click on OpenSSH Authentication Agent again and select Start.\n\nThe OpenSSH Agent will start automatically in future. Now we can create your ssh keys!\n\n\nCreate ssh keys and add them to the ssh-agent\nNote: These instructions will refer to your local PowerShell terminal, as well as the Strudel2 terminal. Pay attention!\nFirstly, we need to generate an ssh key-pair. Open a Powershell terminal, and run the following command:\nssh-keygen -t ed25519\nYou will be asked where you would like to save the keys. Press enter to keep the default location. You will also be asked to enter a passphrase. This will be used to encrypt the private key on your computer. There will be a print out of your key’s fingerprint and location on the screen.\nYou should now have a public and private key in your ~/.ssh directory, which will be named id_ed25519.pub and id_ed25519 respectively. Never share your private key with someone else.\nSecondly, you will need to copy your public key into M3. To do this, run the following command:\ncat ~/.ssh/id_ed25519.pub \nCopy the line of text printed on the screen.\nStart a terminal session in Strudel2 by selecting Terminal from the sidebar on the left-hand side of the screen, and then selecting Run on Login Node. This will open a terminal on M3 in a new tab. Ensure you have copied the public key from before, and run the following command in the Strudel2 terminal:\necho <paste the line you just copied> >> .ssh/authorized_keys\nThis will add your public key to M3 and allow you to authenticate using your new ssh-keys.\nFinally, you will want to add your ssh keys to the ssh agent. You will remember setting a passphrase earlier for accessing your private key. When you add a private key to the ssh-agent, it will ask for your passphrase once, and then remember it until the next time your computer reboots. This means you won’t need to enter a passphrase everytime you log in to M3 with your keys.\nCopy paste the location of your private key, which would have been printed on the screen in your PowerShell terminal earlier. For example, C:\\Users\\username/.ssh/id_ed25519. Run the following in your PowerShell terminal to add the key to your ssh-agent:\nssh-add ~/.ssh/id_ed25519"
  },
  {
    "objectID": "strudelv2_spa/docs/vscode.html#using-your-vs-code-remote-server-do-these-every-time-you-need-a-vs-code-session-on-m3",
    "href": "strudelv2_spa/docs/vscode.html#using-your-vs-code-remote-server-do-these-every-time-you-need-a-vs-code-session-on-m3",
    "title": "MLeRP User Guide Documentation",
    "section": "Using your VS Code remote server (do these every time you need a VS Code session on M3)",
    "text": "Using your VS Code remote server (do these every time you need a VS Code session on M3)\n\n4. Allocate resources for the remote server (i.e. start a job)\nUsing Strudel2 in your web browser, login to CVL and select and JupyterLab based application. Choose the resources you will need, including GPU type, number of CPUs, amount of RAM and time, and click Launch. Wait for the job to begin running under Pending/Running Jupyter Labs.\n\n\n5. Test the connection\n\n\nLinux and Mac\n\n\nOpen a terminal and on Linux and Mac enter:\nssh -l <username> VSCode -o ProxyCommand=\"ssh <username>@m3.massive.org.au /usr/local/sv2/sshnc.sh\"\n\n\n\nWindows\n\n\nOn Windows, open a PowerShell terminal and enter:\nssh -l <username> VSCode -o ProxyCommand=\"ssh.exe <username>@m3.massive.org.au /usr/local/sv2/sshnc.sh\"\n\nVerify that you can connect to M3. If you experience difficulties, this is the point to reach out to help@massive.org.au. This step will also prompt you to accept the public key for M3.\n\n\n6. Start the remote server\nOpen VS Code and open the Command Palette (F1). Select Remote-SSH: Connect to Host from the dropdown menu and select the VSCode host. If you didn’t add the private ssh key to the agent, VS Code will prompt you for your key passphrase It will ask twice (once to the connect to the login nodes, then again to connect to the node running the remote server).\nVS Code will also ask you what type of server it is, select Linux.\nThen you should be good to go."
  },
  {
    "objectID": "strudelv2_spa/docs/vscode.html#faq",
    "href": "strudelv2_spa/docs/vscode.html#faq",
    "title": "MLeRP User Guide Documentation",
    "section": "FAQ:",
    "text": "FAQ:\nQ: I see a message about “Connection Reset by peer”:\nA: This will happen if you fail to authenticate too many times - perhaps typing in your key passphrase incorrectly. Wait 10 minutes, and try again. If you still run into errors please contact the helpdesk at help@massive.org.au\nQ: I keep getting prompted for a password even though I’m using a key\nA: Check that your connection command includes the username everwhere you need it (it should be in there twice, once for the login node and once for the compute node). Also, ensure you added your keys to the ssh-agent to avoid typing your key passphrase every time. You will need to do this after every reboot.\nQ: I got an error message about posix spawn\nA: You’re probably on Windows. Check that you used the Windows specific command line with ssh.exe instead of ssh\nQ: I tried to use the ssh-agent but it says “Permissions mykey are too open.\nA: Private keys should never be shared - ensure only you have read, write, and execute permissions on your private key. Linux/Mac user see This StackOverflow post. Windows Users, see This blog post"
  }
]