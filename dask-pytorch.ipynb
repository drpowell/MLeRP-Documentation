{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and Dask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a cluster and performing some computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/node.py:179: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 42939 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-d1d1528e-a5b4-11ed-9c14-fa163e16ef68</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.SLURMCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://192.168.0.213:42939/status\" target=\"_blank\">http://192.168.0.213:42939/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">SLURMCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">a46be698</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://192.168.0.213:42939/status\" target=\"_blank\">http://192.168.0.213:42939/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-7a8a0a60-b1a9-471f-b88a-8740b7ff16a0</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://192.168.0.213:42643\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://192.168.0.213:42939/status\" target=\"_blank\">http://192.168.0.213:42939/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://192.168.0.213:42643' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client, LocalCluster\n",
    "from dask import delayed\n",
    "import dask\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    memory=\"64g\", processes=1, cores=2\n",
    ")\n",
    "num_nodes = 4\n",
    "\n",
    "cluster.scale(num_nodes)\n",
    "# cluster = LocalCluster(processes=False)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "              1312     batch Jupyter  mhar0048  R      20:59      1 mlerp-node05\n",
      "              1215     batch Jupyter    yiliao  R 3-21:21:03      1 mlerp-node09\n",
      "              1214     batch Jupyter    yiliao  R 3-21:24:39      1 mlerp-node05\n",
      "              1336     batch dask-wor mhar0048  R       0:03      1 mlerp-node05\n",
      "              1337     batch dask-wor mhar0048  R       0:03      1 mlerp-node05\n",
      "              1338     batch dask-wor mhar0048  R       0:03      1 mlerp-node05\n",
      "              1339     batch dask-wor mhar0048  R       0:03      1 mlerp-node09\n"
     ]
    }
   ],
   "source": [
    "# Note how dask spins our jobs up in anticipation for work\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<distributed.deploy.adaptive.Adaptive at 0x7efc80c60790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The adapt method will let us scale out as we need the compute\n",
    "# ...and scale back when we're idle\n",
    "cluster.adapt(minimum=0, maximum=num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "              1312     batch Jupyter  mhar0048  R      21:02      1 mlerp-node05\n",
      "              1215     batch Jupyter    yiliao  R 3-21:21:06      1 mlerp-node09\n",
      "              1214     batch Jupyter    yiliao  R 3-21:24:42      1 mlerp-node05\n",
      "              1336     batch dask-wor mhar0048  R       0:06      1 mlerp-node05\n",
      "              1337     batch dask-wor mhar0048  R       0:06      1 mlerp-node05\n",
      "              1338     batch dask-wor mhar0048  R       0:06      1 mlerp-node05\n",
      "              1339     batch dask-wor mhar0048  R       0:06      1 mlerp-node09\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask has a lovely UI that will let you see how the tasks are being computed\n",
    "# VSCode has an extension for you to connect to this: http://127.0.0.1:8787 (Adjust the port if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 7.45 GiB </td>\n",
       "                        <td> 119.21 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (1000, 1000, 1000) </td>\n",
       "                        <td> (250, 250, 250) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 64 Tasks </td>\n",
       "                        <td> 64 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"250\" height=\"240\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"30\" x2=\"80\" y2=\"100\" />\n",
       "  <line x1=\"10\" y1=\"60\" x2=\"80\" y2=\"130\" />\n",
       "  <line x1=\"10\" y1=\"90\" x2=\"80\" y2=\"160\" />\n",
       "  <line x1=\"10\" y1=\"120\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"137\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"155\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"62\" y2=\"172\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,190.58823529411765 10.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"147\" y2=\"17\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"165\" y2=\"35\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"182\" y2=\"52\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"40\" y1=\"0\" x2=\"110\" y2=\"70\" />\n",
       "  <line x1=\"70\" y1=\"0\" x2=\"140\" y2=\"70\" />\n",
       "  <line x1=\"100\" y1=\"0\" x2=\"170\" y2=\"70\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 130.0,0.0 200.58823529411765,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"100\" x2=\"200\" y2=\"100\" />\n",
       "  <line x1=\"80\" y1=\"130\" x2=\"200\" y2=\"130\" />\n",
       "  <line x1=\"80\" y1=\"160\" x2=\"200\" y2=\"160\" />\n",
       "  <line x1=\"80\" y1=\"190\" x2=\"200\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"110\" y1=\"70\" x2=\"110\" y2=\"190\" />\n",
       "  <line x1=\"140\" y1=\"70\" x2=\"140\" y2=\"190\" />\n",
       "  <line x1=\"170\" y1=\"70\" x2=\"170\" y2=\"190\" />\n",
       "  <line x1=\"200\" y1=\"70\" x2=\"200\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 200.58823529411765,70.58823529411765 200.58823529411765,190.58823529411765 80.58823529411765,190.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"140.588235\" y=\"210.588235\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1000</text>\n",
       "  <text x=\"220.588235\" y=\"130.588235\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,220.588235,130.588235)\">1000</text>\n",
       "  <text x=\"35.294118\" y=\"175.294118\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,175.294118)\">1000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(1000, 1000, 1000), dtype=float64, chunksize=(250, 250, 250), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# da lets us scale out to the cluster more efficiently than npy\n",
    "import dask.array as da\n",
    "x = da.random.random((1000, 1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.95991422e-01, 7.29635940e-01, 2.81710508e-02, ...,\n",
       "         7.80411871e-01, 6.47691554e-01, 7.09732354e-01],\n",
       "        [4.68557888e-02, 1.61744555e-01, 9.68807923e-01, ...,\n",
       "         4.14736245e-01, 3.45848334e-01, 2.94003999e-01],\n",
       "        [8.68459168e-01, 3.31556693e-02, 7.29167989e-01, ...,\n",
       "         2.67547397e-01, 3.62516388e-01, 4.47140395e-01],\n",
       "        ...,\n",
       "        [2.50422490e-01, 1.16683537e-01, 3.64873815e-01, ...,\n",
       "         8.11636480e-01, 2.19410748e-01, 8.34222863e-01],\n",
       "        [5.32683002e-01, 9.89774462e-01, 3.81822225e-01, ...,\n",
       "         5.22320346e-01, 2.45289960e-01, 4.99485662e-01],\n",
       "        [5.73752150e-02, 4.16188506e-01, 2.01487615e-01, ...,\n",
       "         4.78838214e-01, 6.14959854e-01, 7.40204742e-01]],\n",
       "\n",
       "       [[3.78725383e-01, 3.03743484e-02, 4.81805729e-01, ...,\n",
       "         1.80843551e-01, 8.89631808e-01, 2.05693056e-01],\n",
       "        [7.99460633e-01, 9.32727544e-01, 3.78722157e-01, ...,\n",
       "         7.09053389e-02, 1.75157899e-01, 5.05155611e-01],\n",
       "        [8.65631434e-01, 9.18027060e-01, 4.99887763e-02, ...,\n",
       "         6.18883005e-01, 2.22966508e-01, 8.27884551e-01],\n",
       "        ...,\n",
       "        [2.32995447e-03, 2.15908185e-01, 4.60797661e-02, ...,\n",
       "         9.68145729e-01, 2.77669974e-01, 6.59250787e-01],\n",
       "        [6.88440395e-01, 6.04990328e-01, 9.26945571e-01, ...,\n",
       "         6.70756446e-01, 2.68088339e-01, 3.25326417e-01],\n",
       "        [1.38078820e-01, 2.86993920e-01, 6.43549645e-01, ...,\n",
       "         7.43300493e-01, 7.95498691e-01, 8.14184943e-01]],\n",
       "\n",
       "       [[9.79587970e-01, 4.08873141e-02, 1.56707842e-01, ...,\n",
       "         7.17109664e-01, 9.82986235e-01, 4.91476271e-01],\n",
       "        [7.33990844e-01, 1.55286418e-01, 6.97189729e-01, ...,\n",
       "         6.11056062e-01, 5.78386132e-01, 2.73065356e-01],\n",
       "        [1.40541801e-01, 9.48652759e-01, 1.32954909e-01, ...,\n",
       "         5.41427025e-02, 7.31531090e-01, 3.72178575e-01],\n",
       "        ...,\n",
       "        [6.56397051e-01, 3.74079350e-01, 3.23540370e-01, ...,\n",
       "         8.57019235e-01, 6.27163496e-01, 9.56197255e-01],\n",
       "        [3.63306805e-01, 8.11909475e-01, 1.43867791e-02, ...,\n",
       "         8.53823665e-01, 4.41004338e-01, 2.84964322e-01],\n",
       "        [9.58788221e-01, 6.09970052e-01, 4.11134229e-01, ...,\n",
       "         8.86007371e-01, 5.23536623e-01, 2.42334099e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[9.16494878e-01, 4.74560428e-01, 6.13074954e-01, ...,\n",
       "         4.11259094e-01, 2.40629728e-01, 6.64726009e-01],\n",
       "        [5.26593159e-01, 9.67919180e-01, 4.08799440e-02, ...,\n",
       "         7.13878318e-01, 1.53596387e-01, 2.55026319e-01],\n",
       "        [4.72128286e-01, 7.28627577e-01, 2.21399609e-01, ...,\n",
       "         2.69548486e-01, 5.13852836e-01, 7.67521882e-01],\n",
       "        ...,\n",
       "        [3.37596860e-01, 2.13945158e-01, 6.92208819e-01, ...,\n",
       "         9.99404922e-01, 1.81303858e-01, 2.19779590e-01],\n",
       "        [1.88648895e-01, 3.52905317e-02, 5.72917978e-01, ...,\n",
       "         3.65950784e-01, 8.92281683e-01, 4.75157932e-01],\n",
       "        [4.29834278e-01, 9.68265990e-02, 7.04876824e-01, ...,\n",
       "         2.86573557e-01, 4.19366898e-01, 3.36818589e-01]],\n",
       "\n",
       "       [[6.30066084e-01, 5.23169637e-01, 8.16307095e-01, ...,\n",
       "         5.43481385e-01, 6.99754782e-01, 5.36104277e-01],\n",
       "        [3.84039004e-01, 7.67356573e-01, 7.19412252e-01, ...,\n",
       "         2.50284206e-01, 2.46762683e-01, 6.12341428e-01],\n",
       "        [2.03228847e-01, 3.54155928e-02, 2.45428735e-01, ...,\n",
       "         6.51813636e-01, 1.03001393e-01, 1.99887237e-01],\n",
       "        ...,\n",
       "        [2.49806926e-01, 4.20383096e-01, 9.94736109e-01, ...,\n",
       "         4.32979454e-01, 1.80747272e-01, 5.49069364e-01],\n",
       "        [7.46559872e-01, 7.21750372e-01, 1.34988623e-01, ...,\n",
       "         4.66795263e-01, 4.78765941e-01, 5.90751972e-02],\n",
       "        [1.53226205e-01, 2.04428585e-01, 5.40340259e-01, ...,\n",
       "         9.60725170e-01, 9.20562674e-01, 3.86678697e-01]],\n",
       "\n",
       "       [[7.94255205e-01, 6.68940609e-04, 8.84979634e-01, ...,\n",
       "         1.28724329e-01, 5.34185790e-01, 8.73504339e-01],\n",
       "        [6.62631670e-01, 7.71369878e-01, 6.51814722e-01, ...,\n",
       "         3.78966393e-01, 5.26180765e-03, 8.32577003e-01],\n",
       "        [4.21605338e-01, 8.69703631e-01, 6.07284629e-01, ...,\n",
       "         3.46091998e-01, 6.08426325e-01, 5.28212144e-01],\n",
       "        ...,\n",
       "        [3.50952521e-01, 9.59735073e-01, 7.98860573e-01, ...,\n",
       "         8.96289656e-01, 5.02648388e-01, 7.33419660e-02],\n",
       "        [2.35031763e-01, 7.65905226e-01, 3.91048638e-01, ...,\n",
       "         9.89075871e-01, 5.70734611e-02, 4.03795470e-01],\n",
       "        [7.64489884e-02, 7.73490994e-01, 4.04307230e-01, ...,\n",
       "         1.11093071e-01, 1.89086890e-01, 6.19038005e-01]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dask evaluates lazily, retuning 'futures'\n",
    "# they can then be computed later for its value\n",
    "x.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/node.py:179: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44055 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Let's switch to a localcluster for easier active development\n",
    "# This will make all code execute locally\n",
    "# We need to make proccesses=False to allow for multiprocessing inside Dask jobs\n",
    "# for the local cluster to work with PyTorch\n",
    "client.shutdown()\n",
    "cluster = LocalCluster(processes=False)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how Dask works with a typical PyTorch workflow\n",
    "Content adapted from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Define dataset and dataloader\n",
    "batch_size = 1024\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2, multiprocessing_context=mp.get_context(\"fork\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple conv net\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 64, 4 * 64)\n",
    "        self.fc2 = nn.Linear(4 * 64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 00:26:58,981 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train one epoch\n",
    "def train(loader, path=\"./model\", load=False, test=False, error=False):\n",
    "    # Initialise model, optimizer and device\n",
    "    model = Net()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load state from disk so that we can split up the job\n",
    "    if load: \n",
    "        state = torch.load(path)\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        model.to(device)\n",
    "        optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    else:\n",
    "        model.to(device)\n",
    "    \n",
    "    # A typical PyTorch training loop\n",
    "    running_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        # put the inputs on the device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # Force an error\n",
    "        if error:\n",
    "            assert 0 == 1\n",
    "        \n",
    "        # Stop after one batch when testing        \n",
    "        if test: \n",
    "            print(\"When running in a local cluster you can see print statements\")\n",
    "            break\n",
    "    \n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    return running_loss / len(trainloader) if not test else loss.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/worker.py:2845: UserWarning: Large object of size 146.58 MiB detected in task graph: \n",
      "  [<torch.utils.data.dataloader.DataLoader object at 0x7efc80c60520>]\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  warnings.warn(\n",
      "/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When running in a local cluster you can see print statements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.305037021636963"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test our code locally first\n",
    "client.submit(train, trainloader, test=True).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to turn off the nanny to allow for multiprocessing inside Dask jobs for the cluster to work with PyTorch\n",
    "# We can pass in SLURM requirements to ensure we get a GPU for our jobs\n",
    "client.shutdown()\n",
    "cluster = SLURMCluster(\n",
    "    memory=\"64g\", processes=1, cores=2, job_extra_directives=[\"--gres=gpu:1\"], nanny=False\n",
    ")\n",
    "cluster.scale(1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3079774379730225"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test our code on the SLURM cluster\n",
    "# Since this code is executing remotely we won't see our print statements\n",
    "client.submit(train, trainloader, test=True).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 00:29:12,785 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c6552505f6d68617230303438227d/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Dask will raise any errors that the process triggers locally, even when executing remotely\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c6552505f6d68617230303438227d/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m client\u001b[39m.\u001b[39;49msubmit(train, trainloader, error\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m/userdata/mhar0048/miniconda/conda/envs/dask/lib/python3.10/site-packages/distributed/client.py:280\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    279\u001b[0m     typ, exc, tb \u001b[39m=\u001b[39m result\n\u001b[0;32m--> 280\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    281\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcancelled\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    282\u001b[0m     \u001b[39mraise\u001b[39;00m result\n",
      "\u001b[1;32m/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c6552505f6d68617230303438227d/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Force an error\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c6552505f6d68617230303438227d/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c6552505f6d68617230303438227d/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c6552505f6d68617230303438227d/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Stop after one batch when testing        \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c6552505f6d68617230303438227d/home/mhar0048/mlerp-documentation/dask-pytorch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m test: \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dask will raise any errors that the process triggers locally, even when executing remotely\n",
    "client.submit(train, trainloader, error=True).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdc88cde27445d3b7b8ce9548fd3051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss:  2.262\n",
      "epoch: 1 loss:  2.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 00:30:17,778 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "epochs = 2\n",
    "\n",
    "with tqdm(total=(epochs)) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        loss = client.submit(train, trainloader, load=epochs).result()\n",
    "        pbar.update()\n",
    "        pbar.set_postfix(loss=loss)\n",
    "        print(f\"epoch: {epoch} loss: {loss : .3f}\")\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db096b8404a4f1f3e1df0cc89f001e138448327417ef835d10f5a76aa612f160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
