[
  {
    "objectID": "tutorials/0_getting_started.html",
    "href": "tutorials/0_getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Welcome to MLeRP!\nMost users will find that the easiest way to start using MLeRP will be through the Strudel2 Web Interface. Here you can find the Terminal and Jupyter Lab applications we have prepared for you.\nWe have provided some python environments for you so you can get started right away, until you eventually outgrow them and feel like you need more control over the environment. At this point, we can help you to transition to maintaining your own environment with whatever applications and packages you need.\nYour home directory will be set up with a quota based on what you told us you’ll need to do your research. You can use this space to maintain your datasets, programs and code base. Let us know if you need more space and we’ll see what we can do.\nRegardless of what compute you choose, in order to start your research here you will need to be able to import your data and code. If you’re working with relatively small files, then the simplest way could be to use the FTP clients built into Jupyter Lab or VS Code. Otherwise something like rsync may be more appropriate.\nIf you are maintaining your code base through Git, then you will likely need to set up an ssh key and add it to your account before pulling down your repository. To use the right ssh key when using git you will need to set up an ssh config that will look something like this:\nHost github.com\n        User git\n        HostName github.com\n        IdentityFile ~/.ssh/github\nIf you are more comfortable using a Git GUI we reccommend connecting to MLeRP using VS Code and using it’s Git extensions.\nAll of our documentation is programmatically generated with Quarto and is available on GitHub. We encourage users to clone a copy and go through our tutorials at their own pace.\ngit clone git@github.com:mitchellshargreaves-monash/MLeRP-Documentation.git\nIf you feel that our documentation does not serve your use case well and could be improved, we welcome contributions through pull requests."
  },
  {
    "objectID": "tutorials/1_dask_slurm.html",
    "href": "tutorials/1_dask_slurm.html",
    "title": "Dask SLURMClusters",
    "section": "",
    "text": "The MLeRP notebook environment uses Dask SLURMClusters to create a middle ground that has the interactivity of a notebook backed by the power of HPC. This notebook shows how you can use the lion service to use a CPU based notebook session for your basic analysis and code development. Then, when you’re ready to run tests you will use Dask to submit your python functions to the SLURM queue.\nThis enables:\n\nFlexibility to experiment with your dataset interactively\nAbility to change compute requirements such as RAM, size of GPU, number of processes and so on… without ever leaving the notebook environment\nElastic scaling of compute\nEfficient utilisation of the hardware\nReleasing of resources when not in use\n\n\nfrom dask_jobqueue import SLURMCluster\nfrom distributed import Client, LocalCluster\nimport dask\n\n# Point Dask to the SLURM to use as it's back end\ncluster = SLURMCluster(\n    memory=\"64g\", processes=1, cores=8\n)\n\n# Scale out to 4 nodes\nnum_nodes = 4\ncluster.scale(num_nodes)\nclient = Client(cluster)\n\nDask will now spin our jobs up in anticipation for work to the scale that you specify.\nYou can check in on your jobs like you would with any other SLURM job with squeue.\n\n!squeue\n\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               383   BigCats dask-wor mhar0048 PD       0:00      1 (None)\n               382   BigCats dask-wor mhar0048 PD       0:00      1 (None)\n               381   BigCats dask-wor mhar0048 PD       0:00      1 (None)\n               380   BigCats dask-wor mhar0048 PD       0:00      1 (None)\n               373   BigCats DSKS Jup andrewpe  R    1:48:56      1 mlerp-monash-node00\n               375   BigCats DSKS Jup mhar0048  R    1:15:17      1 mlerp-monash-node00\n\n\nAlternatively, we can use the adapt method, which will let us scale out as we need the compute… and scale back when we’re idle letting others use the cluster.\nWe reccommend that you use the adapt method while you’re actively developing your code so that you don’t need to worry about cleaning up after yourself. The scale method can be used when you’re ready to run longer tests with higher utilisation.\n\ncluster.adapt(minimum=0, maximum=num_nodes)\n\n&lt;distributed.deploy.adaptive.Adaptive at 0x7fd919ab7670&gt;\n\n\n\n# You may need to run this cell a few times while waiting for Dask to clean up\n!squeue\n\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               373   BigCats DSKS Jup andrewpe  R    1:48:56      1 mlerp-monash-node00\n               380   BigCats dask-wor mhar0048  R       0:00      1 mlerp-monash-node00\n               381   BigCats dask-wor mhar0048  R       0:00      1 mlerp-monash-node00\n               382   BigCats dask-wor mhar0048  R       0:00      1 mlerp-monash-node00\n               383   BigCats dask-wor mhar0048  R       0:00      1 mlerp-monash-node00\n               375   BigCats DSKS Jup mhar0048  R    1:15:17      1 mlerp-monash-node00\n\n\nDask has a UI that will let you see how the tasks are being computed. You won’t be able to connect to this with your web browser but VSCode and Jupyter have extensions for you to connect to it.\nUse the loopback address: http://127.0.0.1:8787 (Adjust the port to the one listed when you make the client if needed)\nNow let’s define a dask array and perform some computation. Dask arrays are parallelised across your workers nodes so they can be greater than the size of one worker’s memory. Dask evaluates lazily, retuning ‘futures’ which record the tasks needed to be completed in the compute graph. They can be computed later for its value.\nDask also has parallelised implementations of dataframes and collections of objects (called bags). These are written to be as similar as possible to familiar libraries like numpy, pandas and pyspark. You can read more about arrays, dataframes and bags with Dask’s documentation.\n\nimport dask.array as da\nx = da.random.random((1000, 1000, 1000))\nx  # Note how the value of the array hasn't been computed yet\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n7.45 GiB\n126.51 MiB\n\n\nShape\n(1000, 1000, 1000)\n(255, 255, 255)\n\n\nDask graph\n64 chunks in 1 graph layer\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nYou can check squeue while this is running to see the jobs dynamically spinning up to perform the computation.\n\nx[0][0][:10].compute()\n\narray([0.15885921, 0.35993257, 0.08629273, 0.19472071, 0.76723019,\n       0.65067334, 0.33802127, 0.61253251, 0.37531874, 0.06273974])\n\n\nFinally, we can shut down the SLURMCluster now that we’re done with it.\n\n# Shut down the cluster\nclient.shutdown()\n\n2023-08-21 02:44:02,335 - distributed.deploy.adaptive_core - INFO - Adaptive stop"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Say we have a researcher who is a domain expert in a field and is just discovering that machine learning algorithms might be appropriate to accelerate their work.\n\nThey explore online for potential approaches and even find some tutorials that use interactive environments, but quickly discover that dealing with large datasets requires more compute than their laptop can handle so they need to move to a HPC system.\n\nWhile desktop sessions are available they might have prohibitively long queues and have low utilisation of the underlying hardware. They could submit a job to the HPC queue but this leads to long times between iterations. At this stage the researcher is still discovering things about their data, how to clean it and how to analyse it.\n\nThey need an interactive environment so they can develop and debug their algorithms and the algorithms, but still need access to high powered acceleration so they can process their dataset. At the same time we want to improve utilisation of the hardware so that we can serve more users and reduce wait times.\nMLeRP was built to be a middle ground that has the interactivity of a notebook with the power of a HPC environment that can share valuable resources between other users while code isn’t being executed.\n \nNow our new researcher can easily import the same code that they were using on their laptops or with online notebook services straight into a HPC environment without the need to convert it first into a SBATCH script, wait in long HPC queues or load modules for dependencies."
  },
  {
    "objectID": "about.html#what-is-mlerp",
    "href": "about.html#what-is-mlerp",
    "title": "About",
    "section": "",
    "text": "Say we have a researcher who is a domain expert in a field and is just discovering that machine learning algorithms might be appropriate to accelerate their work.\n\nThey explore online for potential approaches and even find some tutorials that use interactive environments, but quickly discover that dealing with large datasets requires more compute than their laptop can handle so they need to move to a HPC system.\n\nWhile desktop sessions are available they might have prohibitively long queues and have low utilisation of the underlying hardware. They could submit a job to the HPC queue but this leads to long times between iterations. At this stage the researcher is still discovering things about their data, how to clean it and how to analyse it.\n\nThey need an interactive environment so they can develop and debug their algorithms and the algorithms, but still need access to high powered acceleration so they can process their dataset. At the same time we want to improve utilisation of the hardware so that we can serve more users and reduce wait times.\nMLeRP was built to be a middle ground that has the interactivity of a notebook with the power of a HPC environment that can share valuable resources between other users while code isn’t being executed.\n \nNow our new researcher can easily import the same code that they were using on their laptops or with online notebook services straight into a HPC environment without the need to convert it first into a SBATCH script, wait in long HPC queues or load modules for dependencies."
  },
  {
    "objectID": "applications/base.html",
    "href": "applications/base.html",
    "title": "Base",
    "section": "",
    "text": "Code\n!pip list\n\n\nPackage                       Version\n----------------------------- ----------------\nabsl-py                       1.4.0\naiohttp                       3.8.5\naiosignal                     1.3.1\nanyio                         3.7.1\nappdirs                       1.4.4\nargon2-cffi                   21.3.0\nargon2-cffi-bindings          21.2.0\narrow                         1.2.3\nasttokens                     2.2.1\nastunparse                    1.6.3\nasync-lru                     2.0.4\nasync-timeout                 4.0.2\nattrs                         23.1.0\nautopep8                      2.0.2\nBabel                         2.12.1\nbackcall                      0.2.0\nbackoff                       2.2.1\nbackports.cached-property     1.0.2\nbackports.functools-lru-cache 1.6.5\nbeautifulsoup4                4.12.2\nbleach                        5.0.1\nblessed                       1.19.1\nblinker                       1.6.2\nblis                          0.7.10\nbokeh                         3.2.1\nbrotlipy                      0.7.0\nbuild                         0.10.0\nCacheControl                  0.12.14\ncached-property               1.5.2\ncachetools                    5.3.1\ncatalogue                     2.0.9\ncertifi                       2023.7.22\ncffi                          1.15.1\ncharset-normalizer            3.2.0\ncleo                          2.0.1\nclick                         8.1.6\ncloudpickle                   2.2.1\ncolorama                      0.4.6\nconfection                    0.1.1\ncontourpy                     1.1.0\ncrashtest                     0.4.1\ncroniter                      1.4.1\ncryptography                  41.0.3\ncycler                        0.11.0\ncymem                         2.0.7\ncytoolz                       0.12.0\ndask                          2023.8.0\ndask-jobqueue                 0.8.2\ndataclasses                   0.8\ndatasets                      2.14.4\ndateutils                     0.6.12\ndebugpy                       1.6.3\ndecorator                     5.1.1\ndeepdiff                      6.3.1\ndefusedxml                    0.7.1\ndill                          0.3.7\ndistlib                       0.3.6\ndistributed                   2023.8.0\ndocker-pycreds                0.4.0\ndulwich                       0.21.5\nentrypoints                   0.4\nexceptiongroup                1.1.2\nexecuting                     1.2.0\nfastai                        2.7.12\nfastapi                       0.101.0\nfastcore                      1.5.29\nfastdownload                  0.0.7\nfastjsonschema                2.18.0\nfastprogress                  1.0.3\nfilelock                      3.12.2\nflatbuffers                   23.5.26\nflit_core                     3.7.1\nfonttools                     4.42.0\nfqdn                          1.5.1\nfrozenlist                    1.4.0\nfsspec                        2023.6.0\ngast                          0.4.0\ngensim                        4.3.1\ngitdb                         4.0.10\nGitPython                     3.1.32\ngmpy2                         2.1.2\ngoogle-auth                   2.22.0\ngoogle-auth-oauthlib          1.0.0\ngoogle-pasta                  0.2.0\ngrpcio                        1.54.3\nh11                           0.14.0\nh5py                          3.9.0\nhtml5lib                      1.1\nhuggingface-hub               0.16.4\nidna                          3.4\nimportlib-metadata            6.8.0\nimportlib-resources           6.0.1\ninquirer                      3.1.3\ninstaller                     0.7.0\nipykernel                     6.16.2\nipython                       8.14.0\nipython-genutils              0.2.0\nipywidgets                    8.0.3\nisoduration                   20.11.0\nitsdangerous                  2.1.2\njaraco.classes                3.3.0\njax                           0.4.14\njaxlib                        0.4.14\njedi                          0.19.0\njeepney                       0.8.0\nJinja2                        3.1.2\njoblib                        1.3.2\njson5                         0.9.14\njsonpointer                   2.0\njsonschema                    4.19.0\njsonschema-specifications     2023.7.1\njupyter                       1.0.0\njupyter_client                7.4.8\njupyter-console               6.4.4\njupyter_core                  5.3.1\njupyter-events                0.7.0\njupyter-lsp                   2.2.0\njupyter_server                2.7.0\njupyter_server_terminals      0.4.4\njupyterlab                    4.0.4\njupyterlab-pygments           0.2.2\njupyterlab_server             2.24.0\njupyterlab-widgets            3.0.8\nkeras                         2.12.0\nKeras-Preprocessing           1.1.2\nkeyring                       23.13.1\nkiwisolver                    1.4.4\nlangcodes                     3.3.0\nlightning                     2.0.6\nlightning-cloud               0.5.37\nlightning-utilities           0.9.0\nlocket                        1.0.0\nlockfile                      0.12.2\nlz4                           4.3.2\nMarkdown                      3.4.4\nmarkdown-it-py                3.0.0\nMarkupSafe                    2.1.3\nmatplotlib                    3.7.2\nmatplotlib-inline             0.1.6\nmdurl                         0.1.0\nmistune                       0.8.4\nml-dtypes                     0.2.0\nmore-itertools                10.1.0\nmpmath                        1.3.0\nmsgpack                       1.0.5\nmultidict                     6.0.4\nmultiprocess                  0.70.15\nmunkres                       1.1.4\nmurmurhash                    1.0.9\nnbclient                      0.6.6\nnbconvert                     6.5.0\nnbformat                      5.9.2\nnest-asyncio                  1.5.6\nnetworkx                      3.1\nnotebook                      6.4.12\nnotebook_shim                 0.2.3\nnumpy                         1.25.2\noauthlib                      3.2.2\nopt-einsum                    3.3.0\nordered-set                   4.1.0\norjson                        3.9.3\noverrides                     7.4.0\npackaging                     23.1\npandas                        2.0.3\npandocfilters                 1.5.0\nparso                         0.8.3\npartd                         1.3.0\npathtools                     0.1.2\npathy                         0.10.2\npexpect                       4.8.0\npickleshare                   0.7.5\nPillow                        10.0.0\npip                           23.2.1\npkginfo                       1.9.6\npkgutil_resolve_name          1.3.10\nplatformdirs                  3.5.1\nplotly                        5.15.0\nply                           3.11\npoetry                        1.5.1\npoetry-core                   1.6.1\npoetry-plugin-export          1.4.0\npooch                         1.7.0\npreshed                       3.0.8\nprometheus-client             0.14.1\nprompt-toolkit                3.0.39\nprotobuf                      4.21.12\npsutil                        5.9.5\nptyprocess                    0.7.0\npure-eval                     0.2.2\npyarrow                       12.0.1\npyasn1                        0.4.8\npyasn1-modules                0.2.7\npycodestyle                   2.11.0\npycparser                     2.21\npydantic                      1.10.12\nPygments                      2.16.1\nPyJWT                         2.8.0\npyOpenSSL                     23.2.0\npyparsing                     3.0.9\npyproject_hooks               1.0.0\nPyQt5                         5.15.9\nPyQt5-sip                     12.12.2\nPySocks                       1.7.1\npython-dateutil               2.8.2\npython-editor                 1.0.4\npython-json-logger            2.0.7\npython-multipart              0.0.6\npytorch-lightning             2.0.6\npytz                          2023.3\npyu2f                         0.1.5\nPyYAML                        6.0\npyzmq                         24.0.1\nqtconsole                     5.3.1\nQtPy                          2.1.0\nrapidfuzz                     2.15.1\nreadchar                      4.0.5.dev0\nreferencing                   0.30.2\nregex                         2023.8.8\nrequests                      2.31.0\nrequests-oauthlib             1.3.1\nrequests-toolbelt             1.0.0\nrfc3339-validator             0.1.4\nrfc3986-validator             0.1.1\nrich                          13.5.1\nrpds-py                       0.9.2\nrsa                           4.9\nsacremoses                    0.0.43\nsafetensors                   0.3.2\nscikit-learn                  1.3.0\nscipy                         1.11.1\nSecretStorage                 3.3.3\nSend2Trash                    1.8.0\nsentry-sdk                    1.29.2\nsetproctitle                  1.3.2\nsetuptools                    65.6.3\nshellingham                   1.5.1\nsip                           6.7.11\nsix                           1.16.0\nsmart-open                    5.2.1\nsmmap                         3.0.5\nsniffio                       1.3.0\nsortedcontainers              2.4.0\nsoupsieve                     2.3.2.post1\nspacy                         3.6.1\nspacy-legacy                  3.0.12\nspacy-loggers                 1.0.4\nsrsly                         2.4.7\nssossh                        0.0.4\nstack-data                    0.6.2\nstarlette                     0.27.0\nstarsessions                  1.3.0\nsympy                         1.12\ntblib                         1.7.0\ntenacity                      8.2.2\ntensorboard                   2.12.3\ntensorboard-data-server       0.7.0\ntensorflow                    2.12.1\ntensorflow-estimator          2.12.0\ntermcolor                     2.3.0\nterminado                     0.15.0\nthinc                         8.1.11\nthreadpoolctl                 3.2.0\ntinycss2                      1.1.1\ntokenizers                    0.13.3\ntoml                          0.10.2\ntomli                         2.0.1\ntomlkit                       0.12.1\ntoolz                         0.12.0\ntorch                         2.0.0.post200\ntorchaudio                    2.0.0\ntorchmetrics                  1.0.3\ntorchvision                   0.15.2a0+072ec57\ntornado                       6.3.2\ntqdm                          4.66.1\ntraitlets                     5.9.0\ntransformers                  4.31.0\ntrove-classifiers             2023.8.7\ntyper                         0.9.0\ntyping_extensions             4.5.0\ntyping-utils                  0.1.0\ntzdata                        2023.3\nunicodedata2                  15.0.0\nuri-template                  1.3.0\nurllib3                       1.26.15\nuvicorn                       0.23.2\nvirtualenv                    20.24.1\nwandb                         0.15.8\nwasabi                        1.1.2\nwcwidth                       0.2.6\nwebcolors                     1.13\nwebencodings                  0.5.1\nwebsocket-client              1.6.1\nwebsockets                    11.0.3\nWerkzeug                      2.3.6\nwheel                         0.41.1\nwidgetsnbextension            4.0.4\nwrapt                         1.15.0\nxgboost                       1.7.6\nxxhash                        0.0.0\nxyzservices                   2023.7.0\nyarl                          1.9.2\nzict                          3.0.0\nzipp                          3.16.2"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Why do I still have to wait in queue?\nDirectly attaching GPU compute to notebooks leads to very low utilisation since the GPU sits idle while you’re debugging your code. By using a queue we are able to service more researchers at once with greater efficiency.\n\n\nIf I still have to wait in queue how is this better than using a traditional HPC environment?\nMLeRP is designed with the idea that a job’s size should be about the size of a cell in a jupyter notebook. With that in mind, the queue has been optimised for short jobs. The maximum wall time of jobs in the MLeRP cluster is 30 minutes. Executing a cell won’t always be immediate but your job should start up pretty quickly. If you need to run a job that runs longer than this, checkpoint your code and split it into multiple jobs. If your code is mature enough that it makes more sense to run a multi-hour or multi-day job, consider using a traditional HPC service. Contact us at mlerphelp@gmail.com if your jobs aren’t starting promptly or your usecase isn’t covered by our platform and you think it should be. ### Why Dask? We looked at a few different options for the MLeRP environment including Sagemaker, Spark and Ray. Ultimately we settled on Dask as the primary tool to interface with the queue because:\n\nSLURM jobs can be submitted with SLURMCluster whenever HPC is needed\nDebugging can be done locally first with LocalCluster with minimal code change\nIt has a familiar syntax as it’s designed as a light wrapper around common libraries like Numpy, Pandas and SciKit-Learn\nHigh level applications can be implemented easily with Scikit-Learn, XGBoost, Skorch, SciKeras\nDask can submit any python function to the SLURM queue allowing the flexibility for bespoke low level applications\nLazy evaluation of functions which allows for asynchronous code\n\n\n\nWill I need to change my code to work with MLeRP?\nYes. Dask unfortunately will not ‘just work’, you will have to do some code change to use it to interface with the cluster. You will also need to get a sense of how to request resources from the cluster with SLURM.\nThat said we believe that this approach of submitting jobs through a python notebook environment will feel more familiar to researchers familiar with the python datascience ecosystem given how Dask is designed as a light wrapper around common libraries like numpy, pandas and Scikit-Learn.\nYou will also be able to work with the cluster without needing to convert your experimental notebook code into a script and maintain the environment with modules like with a traditional cluster.\n\n\nHow much compute should I ask for with my SLURMCluster?\nUnfortunately there is no one size fits all answer to this question. Every research problem has its on demands and constraints so this a bit like asking ‘How long is a piece of string?’.\nIf your work is primarily CPU bound, Dask allows as many tasks as you have CPU cores, but if you are using GPUs you may want to dramatically limit this. One process per GPU could be a good place to start.\nOur GPU compute is split into 10 GiB and 20 GiB sizes and they are on nodes with 640 GiB RAM. If you need a larger GPU flavour for your research, contact us at mlerphelp@gmail.com.\nHere are some default jobs sizes to consider:\nSmall GPU:\nSLURMCluster(memory=\"64g\", processes=1, cores=8, job_extra_directives=[\"--gres=gpu:10g:1\"])\nMulti GPU:\nSLURMCluster(memory=\"128g\", processes=1, cores=8, job_extra_directives=[\"--gres=gpu:10g:2\"])\nMedium GPU:\nSLURMCluster(memory=\"128g\", processes=1, cores=8, job_extra_directives=[\"--gres=gpu:20g:1\"])\nLarge CPU workload:\nSLURMCluster(memory=\"64g\", processes=16, cores=16)\nCPU multiprocessing (increase the scale appropriately with cluster.adapt):\nSLURMCluster(memory=\"4g\", processes=1, cores=1)\n\n\nWhy aren’t my print statements showing up in my jobs?\nPrint statements that are executed on remote machines won’t show up in your notebook. If you are using print statements for debugging, consider using a LocalCluster where they will behave as expected.\nIf you need to record information while code is executing remotely either pass the information back to the notebook when the function returns for it to be printed, or log the output to a file.\nFor more information about using SLURMCluster, visit Dask’s documentation\n\n\nShould I use cluster.scale or cluster.adapt?\nWe recommend that you use the adapt method while you’re actively developing your code so that you don’t need to worry about cleaning up after yourself. The scale method can be used when you’re ready to run longer tests with higher utilisation.\n\n\nHow do I install my favourite python package?\nIf you want to control the python environment we recommend that you install and maintain a miniconda environment in your userdata directory.\n\n\nWhat is a daemonic process and why can’t I run one?\nA daemon is a process that runs as a background process. Dask prefers to control all processes so that it can manage them more gracefully if they fail. If you need to take control of the multiprocessing yourself, you can turn this off with LocalCluster(processes=False) and SLURMCluster(nanny=False)."
  },
  {
    "objectID": "connecting/transferring_files.html",
    "href": "connecting/transferring_files.html",
    "title": "Transferring Files",
    "section": "",
    "text": "MLeRP does not have a dedicated data transfer node. We recommend that you use the login node when managing your files."
  },
  {
    "objectID": "connecting/transferring_files.html#wget",
    "href": "connecting/transferring_files.html#wget",
    "title": "Transferring Files",
    "section": "Wget",
    "text": "Wget\nIf you’re looking for an application or dataset that is publically available from the internet, you will be able to download the data directly into the MLeRP environment with a wget command:\nwget -o destination URL"
  },
  {
    "objectID": "connecting/transferring_files.html#jupyter-lab",
    "href": "connecting/transferring_files.html#jupyter-lab",
    "title": "Transferring Files",
    "section": "Jupyter Lab",
    "text": "Jupyter Lab\nThe easiest option to move files into the MLeRP environment is to use a Jupyter Lab job through Strudle2 since it requires no additional setup. You should be able to drag and drop files into the filetree on the left though it comes with some limitations.\nJupyter Lab does not support transferring folders, so you will need to either zip your files first or create your desired structure first and drop in the files later. You can still select multiple files and drag and drop all of them at once. It also only supports dropping files into the directory you are viewing."
  },
  {
    "objectID": "connecting/transferring_files.html#vs-code",
    "href": "connecting/transferring_files.html#vs-code",
    "title": "Transferring Files",
    "section": "VS Code",
    "text": "VS Code\nIf you have already gone through the steps to connect your VS Code you can drag and drop files into the filetree just like with Jupyter Lab, but VS Code will allow you to drop whole folders at once, including nested folders. You can control where the files and folders end up by hovering over the desired folder."
  },
  {
    "objectID": "connecting/transferring_files.html#rsync",
    "href": "connecting/transferring_files.html#rsync",
    "title": "Transferring Files",
    "section": "rsync",
    "text": "rsync\nIf you’re familiar with the terminal you can use rsync to synchronise file systems and to transfer large amounts of files, with the ability to stop and restart the file transfers. rsync will replicate all files in a folder from one spot to another. It first analyses both file systems to find the difference and then transfers only the changes.\nA typical command to synchronise files from a local folder to MLeRP is:\nFor MLeRP QCIF:\nrsync -auv -e ssh adirectory &lt;username&gt;@mlerp-login0.mlerp.cloud.edu.au:~/destinationdirectory/\nFor MLeRP Monash:\nrsync -auv -e ssh adirectory &lt;username&gt;@monash-mlerp-login0.mlerp.cloud.edu.au:~/destinationdirectory/\nrsync is very powerful and has many options to help transfer data. For example it can delete unwanted files with --delete, compress data before transfer -z or can you let you see what command options might do without actually executing them --dry-run. For more info on rsync try:\nman rsync"
  },
  {
    "objectID": "connecting/vscode.html",
    "href": "connecting/vscode.html",
    "title": "VS Code",
    "section": "",
    "text": "So you’d like to use Visual Studio Code, but you need access to MLeRP too - maybe your data is on the cluster, or you want to run code interactively on a GPU, or you just really love VS Code.\nNo problem. You want a VS Code remote server.\nPlease DO NOT follow the official VS Code documentation for remote servers.\nBy default, if you follow those instructions, you will end up running the VS Code remote server on the login node.\nPlease don’t do this, the login nodes are a shared resource and we would rather allocate a resource just for you.\nInstead, please use this proxy command which will forward your connection to our compute nodes rather than the login node. This will connect to a Jupyter Lab job, or if one doesn’t exist, it will make one for you.\n\n\nLinux and Mac\n\n For MLeRP QCIF:\nProxyCommand=\"ssh &lt;username&gt;@mlerp-login0.mlerp.cloud.edu.au /usr/local/sv2/sshnc.sh\"\nFor MLeRP Monash:\nProxyCommand=\"ssh &lt;username&gt;@monash-mlerp-login0.mlerp.cloud.edu.au /apps/strudel2/strudel_apps/sshnc.sh\"\n\n\n\nWindows\n\n For MLeRP QCIF:\nProxyCommand=\"ssh.exe &lt;username&gt;@mlerp-login0.mlerp.cloud.edu.au /usr/local/sv2/sshnc.sh\"\nFor MLeRP Monash:\nProxyCommand=\"ssh.exe &lt;username&gt;@monash-mlerp-login0.mlerp.cloud.edu.au /apps/strudel2/strudel_apps/sshnc.sh\"\n\nNote that you will need to authenticate with your ssh key/cert, either through your authorized keys file or by generating a cert with ssossh. You can then add it to your agent or provide it to the ssh command (and proxy command) with the -i flag.\nIf you’re not sure how to use a proxy command, read on and we’ll walk you through the steps."
  },
  {
    "objectID": "connecting/vscode.html#introduction",
    "href": "connecting/vscode.html#introduction",
    "title": "VS Code",
    "section": "",
    "text": "So you’d like to use Visual Studio Code, but you need access to MLeRP too - maybe your data is on the cluster, or you want to run code interactively on a GPU, or you just really love VS Code.\nNo problem. You want a VS Code remote server.\nPlease DO NOT follow the official VS Code documentation for remote servers.\nBy default, if you follow those instructions, you will end up running the VS Code remote server on the login node.\nPlease don’t do this, the login nodes are a shared resource and we would rather allocate a resource just for you.\nInstead, please use this proxy command which will forward your connection to our compute nodes rather than the login node. This will connect to a Jupyter Lab job, or if one doesn’t exist, it will make one for you.\n\n\nLinux and Mac\n\n For MLeRP QCIF:\nProxyCommand=\"ssh &lt;username&gt;@mlerp-login0.mlerp.cloud.edu.au /usr/local/sv2/sshnc.sh\"\nFor MLeRP Monash:\nProxyCommand=\"ssh &lt;username&gt;@monash-mlerp-login0.mlerp.cloud.edu.au /apps/strudel2/strudel_apps/sshnc.sh\"\n\n\n\nWindows\n\n For MLeRP QCIF:\nProxyCommand=\"ssh.exe &lt;username&gt;@mlerp-login0.mlerp.cloud.edu.au /usr/local/sv2/sshnc.sh\"\nFor MLeRP Monash:\nProxyCommand=\"ssh.exe &lt;username&gt;@monash-mlerp-login0.mlerp.cloud.edu.au /apps/strudel2/strudel_apps/sshnc.sh\"\n\nNote that you will need to authenticate with your ssh key/cert, either through your authorized keys file or by generating a cert with ssossh. You can then add it to your agent or provide it to the ssh command (and proxy command) with the -i flag.\nIf you’re not sure how to use a proxy command, read on and we’ll walk you through the steps."
  },
  {
    "objectID": "connecting/vscode.html#set-up-instructions-only-do-these-once",
    "href": "connecting/vscode.html#set-up-instructions-only-do-these-once",
    "title": "VS Code",
    "section": "Set-Up Instructions (only do these once)",
    "text": "Set-Up Instructions (only do these once)\n\n1. Prepare VS Code\n\nInstall Visual Studio Code\nInstall the Remote Development Extension Pack\nOpen Settings with Ctrl+, and search for Remote.SSH: Remote Server Listen On Socket. Select the checkbox. \n\n\n\n2. Generate an ssh key and add it to the ssh agent\nYou may be familiar with using passwords to authenticate to MLeRP - ssh keys are another, more secure method of doing this. If you’re unfamiliar with ssh keys, no worries - we’ll step you through how to set one up here.\nIf you want to learn more about why ssh keys are more secure than passwords, this YouTube video does a good job of explaining it, even if it is designed for an AWS user.\nThe process for creating keys and adding them to your ssh agent differs between Windows and Linux/Mac.\n\n\nWindows\n\n\n\nInstall and enable OpenSSH\nBefore we can generate keys on Windows, OpenSSH has to be installed and enabled.\n\nCheck OpenSSH is installed.\n\n\nNavigate to Setting &gt; Apps &gt; Apps & Features\nSelect Optional Features.\nCheck if OpenSSH Client is in the list. If it is, go to step 2.\nIf it isn’t, select the Add a feature button at the top of the page, then select OpenSSH Client, and Install. This will install the OpenSSH Client.\n\n\nOnce installed, you will need to start the OpenSSH Client.\n\n\nUse the Windows search bar to search for and open Services.\nScroll through the list to find OpenSSH Authentication Agent\nDouble click OpenSSH Authentication Agent. Change the Startup type to Automatic and select OK.\nDouble click on OpenSSH Authentication Agent again and select Start.\n\nThe OpenSSH Agent will start automatically in future. Now we can create your ssh keys!\n\n\nCreate ssh keys and add them to the ssh-agent\nNote: These instructions will refer to your local PowerShell terminal, as well as the Strudel2 terminal. Pay attention!\nFirstly, we need to generate an ssh key-pair. Open a Powershell terminal, and run the following command:\nssh-keygen -t ed25519\nYou will be asked where you would like to save the keys. Press enter to keep the default location. You will also be asked to enter a passphrase. This will be used to encrypt the private key on your computer. There will be a print out of your key’s fingerprint and location on the screen.\nYou should now have a public and private key in your ~/.ssh directory, which will be named id_ed25519.pub and id_ed25519 respectively. Never share your private key with someone else.\nSecondly, you will need to copy your public key into MLeRP. To do this, run the following command:\ncat ~/.ssh/id_ed25519.pub \nCopy the line of text printed on the screen.\nStart a terminal session in Strudel2 by selecting Terminal from the sidebar on the left-hand side of the screen, and then selecting Run on Login Node. This will open a terminal on MLeRP in a new tab. Ensure you have copied the public key from before, and run the following command in the Strudel2 terminal:\necho &lt;paste the line you just copied&gt; &gt;&gt; .ssh/authorized_keys\nThis will add your public key to MLeRP and allow you to authenticate using your new ssh-keys.\nFinally, you will want to add your ssh keys to the ssh agent. You will remember setting a passphrase earlier for accessing your private key. When you add a private key to the ssh-agent, it will ask for your passphrase once, and then remember it until the next time your computer reboots. This means you won’t need to enter a passphrase everytime you log in to MLeRP with your keys.\nCopy paste the location of your private key, which would have been printed on the screen in your PowerShell terminal earlier. For example, C:\\Users\\username/.ssh/id_ed25519. Run the following in your PowerShell terminal to add the key to your ssh-agent:\nssh-add ~/.ssh/id_ed25519\n\n\n\nMac and Linux\n\n Note: These instructions will refer to your local terminal, as well as the Strudel2 terminal. Pay attention!\nFirstly, we need to generate an ssh key-pair. Open a local terminal, and run the following command:\nssh-keygen -t ed25519\nYou will be asked where you would like to save the keys. Press enter to keep the default location. You will also be asked to enter a passphrase. This will be used to encrypt the private key on your computer. There will be a print out of your key’s fingerprint and location on the screen.\nYou should now have a public and private key in your ~/.ssh directory, which will be named id_ed25519.pub and id_ed25519 respectively. Never share your private key with someone else.\nSecondly, you will need to copy your public key into MLeRP. To do this, run the following command:\ncat ~/.ssh/id_ed25519.pub\nCopy the line of text printed on the screen.\nStart a terminal session in Strudel2 by selecting Terminal from the sidebar on the left-hand side of the screen, and then selecting Run on Login Node. This will open a terminal on MLeRP in a new tab. Ensure you have copied the public key from before, and run the following command in the Strudel2 terminal:\necho &lt;paste the line you just copied&gt; &gt;&gt; .ssh/authorized_keys\nThis will add your public key to MLeRP and allow you to authenticate using your new ssh-keys. Close the terminal tab and terminate the terminal session.\nFinally, you will want to add your ssh keys to the ssh-agent. You will remember setting a passphrase earlier for accessing your private key. When you add a private key to the ssh-agent, it will ask for your passphrase once, and then remember it until the next time your computer reboots. This means you won’t need to enter a passphrase everytime you log in to MLeRP with your keys.\nTo do this, open a local terminal and run:\nssh-add ~/.ssh/id_ed25519"
  },
  {
    "objectID": "connecting/vscode.html#using-your-vs-code-remote-server-do-these-every-time-you-need-a-vs-code-session-on-var-cluster_name",
    "href": "connecting/vscode.html#using-your-vs-code-remote-server-do-these-every-time-you-need-a-vs-code-session-on-var-cluster_name",
    "title": "VS Code",
    "section": "Using your VS Code remote server (do these every time you need a VS Code session on MLeRP)",
    "text": "Using your VS Code remote server (do these every time you need a VS Code session on MLeRP)\n\n4. Allocate resources for the remote server (i.e. start a job)\nUsing Strudel2 in your web browser, login to CVL and select and Jupyter Lab based application. Choose the resources you will need, including GPU type, number of CPUs, amount of RAM and time, and click Launch. Wait for the job to begin running under Pending/Running Jupyter Labs.\n\n\n5. Test the connection\nOpen a terminal and enter:\nssh -l &lt;username&gt; VSCode -o ProxyCommand=\"&lt;ProxyCommand&gt;\"\nVerify that you can connect to MLeRP. If you experience difficulties, this is the point to reach out to mlerphelp@gmail.com. This step will also prompt you to accept the public key for MLeRP.\n\n\n6. Start the remote server\nOpen VS Code and open the Command Palette (F1). Select Remote-SSH: Connect to Host from the dropdown menu and select the VSCode host. If you didn’t add the private ssh key to the agent, VS Code will prompt you for your key passphrase It will ask twice (once to the connect to the login nodes, then again to connect to the node running the remote server).\nVS Code will also ask you what type of server it is, select Linux.\nThen you should be good to go."
  },
  {
    "objectID": "connecting/vscode.html#faq",
    "href": "connecting/vscode.html#faq",
    "title": "VS Code",
    "section": "FAQ",
    "text": "FAQ\n\nI see a message about “Connection Reset by peer”.\nThis will happen if you fail to authenticate too many times - perhaps typing in your key passphrase incorrectly. Wait 10 minutes, and try again. If you still run into errors please contact the helpdesk at mlerphelp@gmail.com.\n\n\nI keep getting prompted for a password even though I’m using a key.\nCheck that your connection command includes the username everwhere you need it (it should be in there twice, once for the login node and once for the compute node). Also, ensure you added your keys to the ssh-agent to avoid typing your key passphrase every time. You will need to do this after every reboot.\n\n\nI got an error message about posix spawn.\nYou’re probably on Windows. Check that you used the Windows specific command line with ssh.exe instead of ssh\n\n\nI tried to use the ssh-agent but it says “Permissions mykey are too open.\nPrivate keys should never be shared - ensure only you have read, write, and execute permissions on your private key. Linux/Mac user see this StackOverflow post. Windows Users, see this blog post"
  },
  {
    "objectID": "connecting/aaf.html",
    "href": "connecting/aaf.html",
    "title": "AAF",
    "section": "",
    "text": "MLeRP is available to all Australian academic researchers through the Australian Access Federation (AAF). AAF is a not for profit organisation that enables access to online resources, like MLeRP, to Australian educators and researchers.\nThis means that rather than configuring a password to access MLeRP, you will be able to use your organisation’s single sign on to access MLeRP. AAF has been integrated with MLeRP’s web portal, Strudle as well as the SSOSSH key managment system."
  },
  {
    "objectID": "connecting/ssossh.html",
    "href": "connecting/ssossh.html",
    "title": "SSOSSH",
    "section": "",
    "text": "SSOSSH is a tool that brings single sign on to MLeRP. You can use it as an alternative to managing SSH keys when connecting via VS Code or your terminal.\nThe tool works by generating an SSH Certificate rather than the usual approach of adding a public key to the list of authorised keys. This is more secure since the certificate will only be valid for 24 hours, requiring you to go through your organisation’s single sign on again to renew the certificate.\nThis also means that you will no longer need to add your key manually since this will be handled automatically.\nSSOSSH uses a configuration file in your user’s home directory named ~/.authservers.json. This file should be a list of configs for the clusters you wish to connect to.\n\n\nFor MLeRP QCIF:\n\n\n[\n    {\n        \"authorise\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/oauth2/oauth/authorize/choose\",\n        \"base\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/oauth2/\",\n        \"cafingerprint\": \"SHA256:ywDDZvIbx7B2AxujVIsW433fd4Sl1aZ0wl4FFsCRX/E\",\n        \"client_id\": \"Q96kt2Vtw6S78dpORktM81DH\",\n        \"desc\": \"&lt;div&gt;MLeRP&lt;/div&gt;\",\n        \"icon\": null,\n        \"login\": \"mlerp-login0.mlerp.cloud.edu.au\",\n        \"logout\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/oauth2/logout\",\n        \"name\": \"MLeRP_Monash\",\n        \"proxy\": \"/nfs/opt/sv2/sshnc.sh\",\n        \"scope\": \"user:email\",\n        \"sign\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/sign/mlerp_users/api/v1/sign_key\"\n    }\n]\n\n\n\nFor MLeRP Monash:\n\n\n[\n    {\n        \"authorise\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/oauth2/oauth/authorize/choose\",\n        \"base\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/oauth2/\",\n        \"cafingerprint\": \"SHA256:ywDDZvIbx7B2AxujVIsW433fd4Sl1aZ0wl4FFsCRX/E\",\n        \"client_id\": \"Q96kt2Vtw6S78dpORktM81DH\",\n        \"desc\": \"&lt;div&gt;MLeRP&lt;/div&gt;\",\n        \"icon\": null,\n        \"login\": \"monash-mlerp-login0.mlerp.cloud.edu.au\",\n        \"logout\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/oauth2/logout\",\n        \"name\": \"MLeRP_QCIF\",\n        \"proxy\": \"/apps/strudel2/strudel_apps/sshnc.sh\",\n        \"scope\": \"user:email\",\n        \"sign\": \"https://sshauthz.cloud.cvl.org.au/pysshauthz/sign/mlerp_users/api/v1/sign_key\"\n    }\n]\n\nYou can pip install SSOSSH with:\npip install git+https://github.com/HecticHPCSolutions/ssossh\nor if you want to test an experimental feature:\npip install git+https://github.com/HecticHPCSolutions/ssossh@branch-name\nAfter installation you should be able to run ssossh in your favourite terminal to run the program. By default it will create the SSH keys and certificates in your .ssh folder, but you can optionally change this path or have it add direct to your SSH Agent which you may find to be neater. We reccommend using the --setssh flag the first time you use SSOSSH to set up some default SSH configurations to connect to the login node and start up a jupyter session.\nssossh --setssh\nssh MLeRP_Monash_login\nNow that the SSH configurations have been generated you will not need this flag. Each time you can simply rerun the ssossh program to refresh your certificates then use your SSH or VS Code remote session as per normal.\nssossh\nssh MLeRP_Monash_login\nYou can read more about usage details at it’s GitHub Repo."
  },
  {
    "objectID": "connecting/strudel2.html",
    "href": "connecting/strudel2.html",
    "title": "Strudel 2",
    "section": "",
    "text": "Strudel2 is an interactive environment to develop supports two types of jobs. You will be able to reach it from https://mlerp.cloud.edu.au/.\nIt will allow you check the cluster’s usage and your disk quota. You will also be able to launch interactive jobs on the cluster. Strudel2 offers two kinds of interactive jobs: terminal and jupyter lab."
  },
  {
    "objectID": "connecting/strudel2.html#terminal-jobs",
    "href": "connecting/strudel2.html#terminal-jobs",
    "title": "Strudel 2",
    "section": "Terminal Jobs",
    "text": "Terminal Jobs\nTerminal jobs gives you some requested compute attached to a simple text UI. This is ideal if you prefer writing python scripts or running experiments that don’t require a GUI.\nYou can also launch the web terminal attached to the login node if you just need a quick terminal to manage your files, however this is a shared resource with limited compute."
  },
  {
    "objectID": "connecting/strudel2.html#jupyter-lab",
    "href": "connecting/strudel2.html#jupyter-lab",
    "title": "Strudel 2",
    "section": "Jupyter Lab",
    "text": "Jupyter Lab\nJupyter Lab jobs gives you some requested compute attached to a Jupyter Lab IDE. This is ideal if you prefer experimenting in python notebooks or visualising data with a GUI.\nIf you prefer using VS Code to the Jupyter Lab IDE you can also connect to this job with the Remote Development Extension Pack. For more details see our page on connecting via VS Code."
  },
  {
    "objectID": "connecting/strudel2.html#conda-environments",
    "href": "connecting/strudel2.html#conda-environments",
    "title": "Strudel 2",
    "section": "Conda Environments",
    "text": "Conda Environments\nWe’ve provided two environments for you to explore the platform and get started right away:\n\nThe ‘base’ environment which aims is a minimal installation of jupyter lab.\nThe Data Science Kitchen Sink (DSKS) which is an environment with many typical packages for analysis and machine learning\n\nYou can visit the Base and DSKS pages complete lists of packages and versions in each environment.\nIf our conda environments don’t meet your needs, you can consider maintaining your own miniconda or mambaforge installation in your home directory. Once you’re happy with your environment you can point Strudel2 to your environment by edditing your apps.json file which can be found in the .strudel2 folder in your home directory.\nStrudel2 will append the new app you’ve configured below An example apps.json template has been provided below:\n[\n    { \n        \"url\": null,\n        \"name\": \"Custom Jupyter Lab\",\n        \"desc\": \"This is a custom Jupyter Lab container defined in a user's directory.\",\n        \"startscript\": \"#!/bin/bash\\n/apps/strudel2/strudel_apps/jupyter/custom_env.sh /PATH/TO/YOUR/CONDA/INSTALL/ CONDA_ENV_NAME\",\n        \"instactions\": [\n            {\n                \"name\": \"Connect\",\n                \"paramscmd\": \"/apps/strudel2/strudel_apps/jupyter/params_cmd.py {jobid} 2&gt;/dev/null\",\n                \"client\": {\n                    \"cmd\": null,\n                    \"redir\": \"?token={token}\"\n                },\n                \"states\": [\n                    \"RUNNING\"\n                ]\n            },\n            {\n                \"name\": \"Remove log\",\n                \"paramscmd\": \"rm ~/.strudel2-*{jobid}.out ; echo []\",\n                \"client\": null,\n                \"notunnel\": true,\n                \"states\": [\n                    \"Finished\"\n                ]\n            }\n        ],\n        \"localbind\": true,\n        \"applist\": null\n    }\n]\nYou can edit the name and description fields to suit your environment for your reference. You will also need to edit the path to your conda installation and the environment name you’d like to activate. Note that to use the strudel Jupyter Lab application you will need to make sure the jupyterlab package is installed in your environment. If you’d like to point Strudel2 to more than one environment, simply append another object to the list."
  },
  {
    "objectID": "connecting/strudel2.html#faq",
    "href": "connecting/strudel2.html#faq",
    "title": "Strudel 2",
    "section": "FAQ",
    "text": "FAQ\n\nWhy can’t I connect to Jupyter Lab?\nJupyter Lab needs to be able to create files to run properly. If your disk quota is completely full, it will create empty files that will confuse the system. To fix this, ssh into the cluster or use a terminal job to reduce your disk usage back under quota, then delete the empty files which will be found at /home/&lt;USERNAME&gt;/.local/share/jupyter/runtime.\nThere is also a known bug where Brave’s ‘shield’ feature will block the Strudel2 Jupyter application. To bypass this, you’ll need to turn off ‘shields’ for both the strudel2 home page and the strudel2 api."
  },
  {
    "objectID": "applications/dsks.html",
    "href": "applications/dsks.html",
    "title": "Data Science Kitchen Sink (DSKS)",
    "section": "",
    "text": "Code\n!conda list\n\n\n# packages in environment at /apps/mambaforge/envs/dsks:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    fastchan\n_openmp_mutex             4.5                  2_kmp_llvm    fastchan\n_py-xgboost-mutex         2.0                       gpu_0    conda-forge\nabsl-py                   1.4.0              pyhd8ed1ab_0    conda-forge\naiohttp                   3.8.5           py310h2372a71_0    fastchan\naiosignal                 1.3.1              pyhd8ed1ab_0    fastchan\nalsa-lib                  1.2.9                hd590300_0    conda-forge\nanyio                     3.7.1              pyhd8ed1ab_0    fastchan\nappdirs                   1.4.4              pyh9f0ad1d_0    fastchan\nargon2-cffi               21.3.0             pyhd8ed1ab_0    fastchan\nargon2-cffi-bindings      21.2.0          py310h5764c6d_2    fastchan\narrow                     1.2.3              pyhd8ed1ab_0    conda-forge\nasttokens                 2.2.1              pyhd8ed1ab_0    fastchan\nastunparse                1.6.3              pyhd8ed1ab_0    fastchan\nasync-lru                 2.0.4              pyhd8ed1ab_0    conda-forge\nasync-timeout             4.0.2              pyhd8ed1ab_0    fastchan\nattr                      2.5.1                h166bdaf_0    fastchan\nattrs                     23.1.0             pyh71513ae_0    fastchan\nautopep8                  2.0.2              pyhd8ed1ab_0    conda-forge\naws-c-auth                0.6.28               he57a670_2    fastchan\naws-c-cal                 0.5.27               hf85dbcb_0    fastchan\naws-c-common              0.8.20               hd590300_0    fastchan\naws-c-compression         0.2.17               h4b87b72_0    fastchan\naws-c-event-stream        0.3.0                hba2fca9_2    fastchan\naws-c-http                0.7.8                h29e0427_1    fastchan\naws-c-io                  0.13.22              h5116816_0    conda-forge\naws-c-mqtt                0.8.12               hb5295a0_0    conda-forge\naws-c-s3                  0.3.4                ha13a167_1    fastchan\naws-c-sdkutils            0.1.10               h4b87b72_0    fastchan\naws-checksums             0.1.15               h4b87b72_0    conda-forge\naws-crt-cpp               0.20.2               h5e6ac67_7    fastchan\naws-sdk-cpp               1.10.57             h8101662_14    conda-forge\nbabel                     2.12.1             pyhd8ed1ab_1    conda-forge\nbackcall                  0.2.0              pyh9f0ad1d_0    fastchan\nbackoff                   2.2.1              pyhd8ed1ab_0    conda-forge\nbackports                 1.0                        py_2    fastchan\nbackports.cached-property 1.0.2              pyhd8ed1ab_0    conda-forge\nbackports.functools_lru_cache 1.6.5              pyhd8ed1ab_0    fastchan\nbeautifulsoup4            4.12.2             pyha770c72_0    fastchan\nbleach                    5.0.1              pyhd8ed1ab_0    fastchan\nblessed                   1.19.1             pyhe4f9e05_2    conda-forge\nblinker                   1.6.2              pyhd8ed1ab_0    conda-forge\nbokeh                     3.2.1              pyhd8ed1ab_0    conda-forge\nbrotli                    1.0.9                h166bdaf_7    fastchan\nbrotli-bin                1.0.9                h166bdaf_7    fastchan\nbrotlipy                  0.7.0           py310h5764c6d_1004    fastchan\nbzip2                     1.0.8                h7f98852_4    fastchan\nc-ares                    1.19.1               hd590300_0    fastchan\nca-certificates           2023.7.22            hbcca054_0    conda-forge\ncachecontrol              0.12.14            pyhd8ed1ab_0    conda-forge\ncachecontrol-with-filecache 0.12.14            pyhd8ed1ab_0    conda-forge\ncached-property           1.5.2                hd8ed1ab_1    conda-forge\ncached_property           1.5.2              pyha770c72_1    conda-forge\ncachetools                5.3.1              pyhd8ed1ab_0    conda-forge\ncairo                     1.16.0            hbbf8b49_1016    conda-forge\ncatalogue                 2.0.9           py310hff52083_0    fastchan\ncertifi                   2023.7.22          pyhd8ed1ab_0    fastchan\ncffi                      1.15.1          py310h255011f_0    fastchan\ncharset-normalizer        3.2.0              pyhd8ed1ab_0    fastchan\ncleo                      2.0.1              pyhd8ed1ab_0    conda-forge\nclick                     8.1.6           unix_pyh707e725_0    fastchan\ncloudpickle               2.2.1              pyhd8ed1ab_0    fastchan\ncolorama                  0.4.6              pyhd8ed1ab_0    fastchan\nconfection                0.1.1           py310hfdc917e_0    fastchan\ncontourpy                 1.1.0           py310hd41b1e2_0    fastchan\ncrashtest                 0.4.1              pyhd8ed1ab_0    conda-forge\ncroniter                  1.4.1              pyhd8ed1ab_0    conda-forge\ncryptography              41.0.3          py310h75e40e8_0    fastchan\ncuda-cudart               11.8.89                       0    nvidia\ncuda-cupti                11.8.87                       0    nvidia\ncuda-libraries            11.8.0                        0    nvidia\ncuda-nvrtc                11.8.89                       0    nvidia\ncuda-nvtx                 11.8.86                       0    nvidia\ncuda-runtime              11.8.0                        0    nvidia\ncuda-version              11.8                 h70ddcb2_2    conda-forge\ncudatoolkit               11.8.0              h37601d7_10    fastchan\ncudnn                     8.8.0.121            h0800d71_1    conda-forge\ncupy                      12.1.0          py310h53f8385_1    conda-forge\ncycler                    0.11.0             pyhd8ed1ab_0    fastchan\ncymem                     2.0.7           py310hd8f1fbe_0    fastchan\ncython-blis               0.7.10          py310h278f3c1_1    fastchan\ncytoolz                   0.12.0          py310h5764c6d_0    fastchan\ndask                      2023.8.1           pyhd8ed1ab_0    conda-forge\ndask-core                 2023.8.1           pyhd8ed1ab_0    conda-forge\ndask-jobqueue             0.8.2              pyhd8ed1ab_0    conda-forge\ndataclasses               0.8                pyhc8e2a94_3    fastchan\ndatasets                  2.14.4                     py_0    huggingface\ndateutils                 0.6.12                     py_0    conda-forge\ndbus                      1.13.6               h5008d03_3    fastchan\ndebugpy                   1.6.3           py310hd8f1fbe_0    fastchan\ndecorator                 5.1.1              pyhd8ed1ab_0    fastchan\ndeepdiff                  6.3.1              pyhd8ed1ab_0    conda-forge\ndefusedxml                0.7.1              pyhd8ed1ab_0    fastchan\ndill                      0.3.7              pyhd8ed1ab_0    fastchan\ndistlib                   0.3.6              pyhd8ed1ab_0    fastchan\ndistributed               2023.8.1           pyhd8ed1ab_0    conda-forge\ndocker-pycreds            0.4.0                      py_0    conda-forge\ndulwich                   0.21.5          py310h2372a71_0    conda-forge\nentrypoints               0.4                pyhd8ed1ab_0    fastchan\nexceptiongroup            1.1.2              pyhd8ed1ab_0    fastchan\nexecuting                 1.2.0              pyhd8ed1ab_0    fastchan\nexpat                     2.5.0                h27087fc_0    fastchan\nfastai                    2.7.12                     py_0    fastchan\nfastapi                   0.101.0            pyhd8ed1ab_0    conda-forge\nfastcore                  1.5.29                     py_0    fastchan\nfastdownload              0.0.7                      py_0    fastchan\nfastprogress              1.0.3                      py_0    fastchan\nfastrlock                 0.8             py310hd8f1fbe_3    conda-forge\nfilelock                  3.12.2             pyhd8ed1ab_0    fastchan\nflatbuffers               23.5.26              h59595ed_1    conda-forge\nflit-core                 3.7.1              pyhd8ed1ab_0    fastchan\nfont-ttf-dejavu-sans-mono 2.37                 hab24e00_0    fastchan\nfont-ttf-inconsolata      3.000                h77eed37_0    fastchan\nfont-ttf-source-code-pro  2.038                h77eed37_0    fastchan\nfont-ttf-ubuntu           0.83                 hab24e00_0    fastchan\nfontconfig                2.14.2               h14ed4e7_0    fastchan\nfonts-conda-ecosystem     1                             0    fastchan\nfonts-conda-forge         1                             0    fastchan\nfonttools                 4.42.0          py310h2372a71_0    fastchan\nfqdn                      1.5.1              pyhd8ed1ab_0    conda-forge\nfreetype                  2.12.1               hca18f0e_0    fastchan\nfrozenlist                1.4.0           py310h2372a71_0    fastchan\nfsspec                    2023.6.0           pyh1a96a4e_0    fastchan\ngast                      0.4.0              pyh9f0ad1d_0    conda-forge\ngensim                    4.3.1           py310h9b08913_1    conda-forge\ngettext                   0.21.1               h27087fc_0    fastchan\ngflags                    2.2.2             he1b5a44_1004    fastchan\ngiflib                    5.2.1                h36c2ea0_2    fastchan\ngitdb                     4.0.10             pyhd8ed1ab_0    conda-forge\ngitpython                 3.1.32             pyhd8ed1ab_0    conda-forge\nglib                      2.76.4               hfc55251_0    fastchan\nglib-tools                2.76.4               hfc55251_0    fastchan\nglog                      0.6.0                h6f12383_0    fastchan\ngmp                       6.2.1                h58526e2_0    fastchan\ngmpy2                     2.1.2           py310h3ec546c_1    fastchan\ngoogle-auth               2.22.0             pyh1a96a4e_0    conda-forge\ngoogle-auth-oauthlib      1.0.0              pyhd8ed1ab_1    conda-forge\ngoogle-pasta              0.2.0              pyh8c360ce_0    conda-forge\ngraphite2                 1.3.13            h58526e2_1001    fastchan\ngrpcio                    1.54.3          py310heca2aa9_0    conda-forge\ngst-plugins-base          1.22.5               hf7dbed1_0    conda-forge\ngstreamer                 1.22.5               h98fc4e7_0    conda-forge\nh11                       0.14.0             pyhd8ed1ab_0    conda-forge\nh5py                      3.9.0           nompi_py310hcca72df_101    conda-forge\nharfbuzz                  7.3.0                hdb3a94d_0    fastchan\nhdf5                      1.14.1          nompi_h4f84152_100    conda-forge\nhtml5lib                  1.1                pyh9f0ad1d_0    conda-forge\nhuggingface_hub           0.16.4                     py_0    huggingface\nhupper                    1.12               pyhd8ed1ab_0    conda-forge\nicu                       72.1                 hcb278e6_0    fastchan\nidna                      3.4                pyhd8ed1ab_0    fastchan\nimportlib-metadata        6.8.0              pyha770c72_0    fastchan\nimportlib_metadata        6.8.0                hd8ed1ab_0    fastchan\nimportlib_resources       6.0.1              pyhd8ed1ab_0    fastchan\ninquirer                  3.1.3              pyhd8ed1ab_0    conda-forge\nipykernel                 6.16.2             pyh210e3f2_0    fastchan\nipython                   8.14.0             pyh41d4057_0    fastchan\nipython_genutils          0.2.0                      py_1    fastchan\nipywidgets                8.0.3              pyhd8ed1ab_0    fastchan\nisoduration               20.11.0            pyhd8ed1ab_0    conda-forge\nitsdangerous              2.1.2              pyhd8ed1ab_0    conda-forge\njaraco.classes            3.3.0              pyhd8ed1ab_0    conda-forge\njax                       0.4.14             pyhd8ed1ab_1    conda-forge\njaxlib                    0.4.14          cpu_py310h67d73b5_1    conda-forge\njedi                      0.19.0             pyhd8ed1ab_0    fastchan\njeepney                   0.8.0              pyhd8ed1ab_0    conda-forge\njinja2                    3.1.2              pyhd8ed1ab_0    fastchan\njoblib                    1.3.2              pyhd8ed1ab_0    fastchan\njson5                     0.9.14             pyhd8ed1ab_0    fastchan\njsonpointer               2.0                        py_0    fastchan\njsonschema                4.19.0             pyhd8ed1ab_0    fastchan\njsonschema-specifications 2023.7.1           pyhd8ed1ab_0    fastchan\njsonschema-with-format-nongpl 4.19.0             pyhd8ed1ab_0    conda-forge\njupyter                   1.0.0           py310hff52083_7    fastchan\njupyter-lsp               2.2.0              pyhd8ed1ab_0    conda-forge\njupyter_client            7.4.8              pyhd8ed1ab_0    fastchan\njupyter_console           6.4.4              pyhd8ed1ab_0    fastchan\njupyter_core              5.3.1           py310hff52083_0    fastchan\njupyter_events            0.7.0              pyhd8ed1ab_2    conda-forge\njupyter_server            2.7.0              pyhd8ed1ab_0    conda-forge\njupyter_server_terminals  0.4.4              pyhd8ed1ab_1    conda-forge\njupyterlab                4.0.5              pyhd8ed1ab_0    conda-forge\njupyterlab_pygments       0.2.2              pyhd8ed1ab_0    fastchan\njupyterlab_server         2.24.0             pyhd8ed1ab_0    conda-forge\njupyterlab_widgets        3.0.8              pyhd8ed1ab_0    conda-forge\nkeras                     2.12.0             pyhd8ed1ab_0    conda-forge\nkeras-preprocessing       1.1.2              pyhd8ed1ab_0    conda-forge\nkeyring                   23.13.1         py310hff52083_0    conda-forge\nkeyutils                  1.6.1                h166bdaf_0    fastchan\nkiwisolver                1.4.4           py310hbf28c38_0    fastchan\nkrb5                      1.21.1               h659d440_0    fastchan\nlame                      3.100             h7f98852_1001    fastchan\nlangcodes                 3.3.0              pyhd8ed1ab_0    fastchan\nlcms2                     2.15                 haa2dc70_1    fastchan\nld_impl_linux-64          2.40                 h41732ed_0    conda-forge\nlerc                      4.0.0                h27087fc_0    fastchan\nlibabseil                 20230125.3      cxx17_h59595ed_0    fastchan\nlibaec                    1.0.6                h9c3ff4c_0    fastchan\nlibarrow                  12.0.1           hc410076_0_cpu    fastchan\nlibblas                   3.9.0           14_linux64_openblas    fastchan\nlibbrotlicommon           1.0.9                h166bdaf_7    fastchan\nlibbrotlidec              1.0.9                h166bdaf_7    fastchan\nlibbrotlienc              1.0.9                h166bdaf_7    fastchan\nlibcap                    2.67                 he9d0100_0    fastchan\nlibcblas                  3.9.0           14_linux64_openblas    fastchan\nlibclang                  15.0.7          default_had23c3d_0    fastchan\nlibclang13                15.0.7          default_h3e3d535_0    fastchan\nlibcrc32c                 1.1.2                h9c3ff4c_0    fastchan\nlibcublas                 11.11.3.6                     0    nvidia\nlibcufft                  10.9.0.58                     0    nvidia\nlibcufile                 1.7.1.12                      0    nvidia\nlibcups                   2.3.3                h4637d8d_4    conda-forge\nlibcurand                 10.3.3.129                    0    nvidia\nlibcurl                   8.2.1                hca28451_0    fastchan\nlibcusolver               11.4.1.48                     0    nvidia\nlibcusparse               11.7.5.86                     0    nvidia\nlibdeflate                1.18                 h0b41bf4_0    fastchan\nlibedit                   3.1.20191231         he28a2e2_2    conda-forge\nlibev                     4.33                 h516909a_1    fastchan\nlibevent                  2.1.12               h3358134_0    fastchan\nlibexpat                  2.5.0                hcb278e6_1    fastchan\nlibffi                    3.4.2                h7f98852_5    conda-forge\nlibflac                   1.4.3                h59595ed_0    fastchan\nlibgcc-ng                 12.1.0              h8d9b700_16    fastchan\nlibgcrypt                 1.10.1               h166bdaf_0    fastchan\nlibgfortran-ng            13.1.0               h69a702a_0    fastchan\nlibgfortran5              13.1.0               h15d22d2_0    fastchan\nlibglib                   2.76.4               hebfc3b9_0    fastchan\nlibgoogle-cloud           2.12.0               hac9eb74_0    fastchan\nlibgpg-error              1.47                 h71f35ed_0    fastchan\nlibgrpc                   1.54.3               hb20ce57_0    conda-forge\nlibhwloc                  2.9.2           nocuda_h7313eea_1008    fastchan\nlibiconv                  1.17                 h166bdaf_0    fastchan\nlibjpeg-turbo             2.1.5.1              h0b41bf4_0    fastchan\nliblapack                 3.9.0           14_linux64_openblas    fastchan\nlibllvm15                 15.0.7               h5cf9203_3    conda-forge\nlibmagma                  2.7.1                hc72dce7_3    conda-forge\nlibmagma_sparse           2.7.1                hc72dce7_4    conda-forge\nlibnghttp2                1.52.0               h61bc06f_0    fastchan\nlibnpp                    11.8.0.86                     0    nvidia\nlibnsl                    2.0.0                h7f98852_0    conda-forge\nlibnuma                   2.0.16               h0b41bf4_1    fastchan\nlibnvjpeg                 11.9.0.86                     0    nvidia\nlibogg                    1.3.4                h7f98852_1    fastchan\nlibopenblas               0.3.20          pthreads_h78a6416_1    conda-forge\nlibopus                   1.3.1                h7f98852_1    fastchan\nlibpng                    1.6.39               h753d276_0    fastchan\nlibpq                     15.4                 hfc447b1_0    conda-forge\nlibprotobuf               3.21.12              h3eb15da_0    fastchan\nlibsndfile                1.2.0                hb75c966_0    fastchan\nlibsodium                 1.0.18               h36c2ea0_1    fastchan\nlibsqlite                 3.42.0               h2797004_0    conda-forge\nlibssh2                   1.11.0               h0841786_0    fastchan\nlibstdcxx-ng              13.1.0               hfd8a6a1_0    fastchan\nlibsystemd0               253                  h8c4010b_1    fastchan\nlibthrift                 0.18.1               h8fd135c_2    conda-forge\nlibtiff                   4.5.1                h8b53f26_0    fastchan\nlibutf8proc               2.8.0                h166bdaf_0    fastchan\nlibuuid                   2.38.1               h0b41bf4_0    fastchan\nlibvorbis                 1.3.7                h9c3ff4c_0    fastchan\nlibwebp-base              1.3.1                hd590300_0    fastchan\nlibxcb                    1.15                 h0b41bf4_0    fastchan\nlibxgboost                1.7.6           cuda112h7a395e5_0    conda-forge\nlibxkbcommon              1.5.0                h5d7e998_3    conda-forge\nlibxml2                   2.11.5               h0d562d8_0    fastchan\nlibzlib                   1.2.13               hd590300_5    conda-forge\nlightning                 2.0.7              pyhd8ed1ab_0    conda-forge\nlightning-cloud           0.5.37             pyhd8ed1ab_0    conda-forge\nlightning-utilities       0.9.0              pyhd8ed1ab_0    conda-forge\nllvm-openmp               16.0.6               h4dfa4b3_0    conda-forge\nlocket                    1.0.0              pyhd8ed1ab_0    fastchan\nlockfile                  0.12.2                     py_1    conda-forge\nlz4                       4.3.2           py310h0cfdcf0_0    conda-forge\nlz4-c                     1.9.4                hcb278e6_0    fastchan\nmagma                     2.7.1                ha770c72_4    conda-forge\nmarkdown                  3.4.4              pyhd8ed1ab_0    conda-forge\nmarkdown-it-py            3.0.0              pyhd8ed1ab_0    fastchan\nmarkupsafe                2.1.3           py310h2372a71_0    fastchan\nmatplotlib                3.7.2           py310hff52083_0    fastchan\nmatplotlib-base           3.7.2           py310hf38f957_0    fastchan\nmatplotlib-inline         0.1.6              pyhd8ed1ab_0    fastchan\nmdurl                     0.1.0              pyhd8ed1ab_0    fastchan\nmistune                   0.8.4           py310h6acc77f_1005    fastchan\nmkl                       2022.2.1         h84fe81f_16997    conda-forge\nml_dtypes                 0.2.0           py310h7cbd5c2_1    conda-forge\nmore-itertools            10.1.0             pyhd8ed1ab_0    conda-forge\nmpc                       1.3.1                hfe3b2da_0    fastchan\nmpfr                      4.2.0                hb012696_0    fastchan\nmpg123                    1.31.3               hcb278e6_0    fastchan\nmpmath                    1.3.0              pyhd8ed1ab_0    fastchan\nmsgpack-python            1.0.5           py310hdf3cbec_0    conda-forge\nmultidict                 6.0.4           py310h1fa729e_0    fastchan\nmultiprocess              0.70.15         py310h2372a71_0    fastchan\nmunkres                   1.1.4              pyh9f0ad1d_0    fastchan\nmurmurhash                1.0.9           py310hd8f1fbe_0    fastchan\nmysql-common              8.0.33               hf1915f5_0    fastchan\nmysql-libs                8.0.33               hca2cd23_0    fastchan\nnbclient                  0.6.6              pyhd8ed1ab_0    fastchan\nnbconvert                 6.5.0              pyhd8ed1ab_0    fastchan\nnbconvert-core            6.5.0              pyhd8ed1ab_0    fastchan\nnbconvert-pandoc          6.5.0              pyhd8ed1ab_0    fastchan\nnbformat                  5.9.2              pyhd8ed1ab_0    fastchan\nnccl                      2.18.3.1             h12f7317_0    conda-forge\nncurses                   6.4                  hcb278e6_0    conda-forge\nnest-asyncio              1.5.6              pyhd8ed1ab_0    fastchan\nnetworkx                  3.1                pyhd8ed1ab_0    fastchan\nnotebook                  6.4.12             pyha770c72_0    fastchan\nnotebook-shim             0.2.3              pyhd8ed1ab_0    conda-forge\nnspr                      4.35                 h27087fc_0    fastchan\nnss                       3.89                 he45b914_0    fastchan\nnumpy                     1.25.2          py310ha4c1d20_0    fastchan\noauthlib                  3.2.2              pyhd8ed1ab_0    conda-forge\nopenjpeg                  2.5.0                h7d73246_1    fastchan\nopenssl                   3.1.2                hd590300_0    conda-forge\nopt_einsum                3.3.0              pyhd8ed1ab_1    conda-forge\norc                       1.8.4                h2f23424_0    fastchan\nordered-set               4.1.0              pyhd8ed1ab_0    conda-forge\norjson                    3.9.3           py310h1e2579a_0    conda-forge\noverrides                 7.4.0              pyhd8ed1ab_0    conda-forge\npackaging                 23.1               pyhd8ed1ab_0    fastchan\npandas                    2.0.3           py310h7cbd5c2_0    fastchan\npandoc                    2.18                 ha770c72_0    fastchan\npandocfilters             1.5.0              pyhd8ed1ab_0    fastchan\nparso                     0.8.3              pyhd8ed1ab_0    fastchan\npartd                     1.3.0              pyhd8ed1ab_0    fastchan\npastedeploy               3.0.1              pyhd8ed1ab_0    conda-forge\npathtools                 0.1.2                      py_1    conda-forge\npathy                     0.10.2             pyhd8ed1ab_0    fastchan\npcre2                     10.40                hc3806b6_0    fastchan\npexpect                   4.8.0              pyh9f0ad1d_2    fastchan\npickleshare               0.7.5                   py_1003    fastchan\npillow                    10.0.0          py310h582fbeb_0    fastchan\npip                       23.2.1             pyhd8ed1ab_0    conda-forge\npixman                    0.40.0               h36c2ea0_0    fastchan\npkginfo                   1.9.6              pyhd8ed1ab_0    fastchan\npkgutil-resolve-name      1.3.10             pyhd8ed1ab_0    fastchan\nplaster                   1.0                        py_0    conda-forge\nplaster_pastedeploy       0.7                        py_0    conda-forge\nplatformdirs              3.5.1              pyhd8ed1ab_0    fastchan\nplotly                    5.16.1                     py_0    plotly\nply                       3.11                       py_1    fastchan\npoetry                    1.5.1           linux_pyhd8ed1ab_0    conda-forge\npoetry-core               1.6.1              pyhd8ed1ab_0    conda-forge\npoetry-plugin-export      1.4.0              pyhd8ed1ab_0    conda-forge\npooch                     1.7.0              pyhd8ed1ab_0    fastchan\npreshed                   3.0.8           py310hd8f1fbe_0    fastchan\nprometheus_client         0.14.1             pyhd8ed1ab_0    fastchan\nprompt-toolkit            3.0.39             pyha770c72_0    fastchan\nprompt_toolkit            3.0.39               hd8ed1ab_0    fastchan\nprotobuf                  4.21.12         py310heca2aa9_0    conda-forge\npsutil                    5.9.5           py310h1fa729e_0    fastchan\npthread-stubs             0.4               h36c2ea0_1001    fastchan\nptyprocess                0.7.0              pyhd3deb0d_0    fastchan\npulseaudio-client         16.1                 h5195f5e_3    fastchan\npure_eval                 0.2.2              pyhd8ed1ab_0    fastchan\npy-xgboost                1.7.6           cuda112py310h898796e_0    conda-forge\npy-xgboost-gpu            1.7.6           py310h9840055_0    conda-forge\npyarrow                   12.0.1          py310h0576679_0_cpu    fastchan\npyasn1                    0.4.8                      py_0    conda-forge\npyasn1-modules            0.2.7                      py_0    conda-forge\npycodestyle               2.11.0             pyhd8ed1ab_0    conda-forge\npycparser                 2.21               pyhd8ed1ab_0    fastchan\npydantic                  1.10.12         py310h2372a71_0    fastchan\npygments                  2.16.1             pyhd8ed1ab_0    fastchan\npyjwt                     2.8.0              pyhd8ed1ab_0    conda-forge\npyopenssl                 23.2.0             pyhd8ed1ab_1    fastchan\npyparsing                 3.0.9              pyhd8ed1ab_0    fastchan\npyproject_hooks           1.0.0              pyhd8ed1ab_0    conda-forge\npyqt                      5.15.9          py310h04931ad_4    fastchan\npyqt5-sip                 12.12.2         py310hc6cd4ac_4    fastchan\npyramid                   2.0.1              pyhd8ed1ab_0    conda-forge\npysocks                   1.7.1           py310hff52083_5    fastchan\npython                    3.10.8          h4a9ceb5_0_cpython    fastchan\npython-build              0.10.0             pyhd8ed1ab_1    conda-forge\npython-dateutil           2.8.2              pyhd8ed1ab_0    fastchan\npython-editor             1.0.4                      py_0    conda-forge\npython-fastjsonschema     2.18.0             pyhd8ed1ab_0    fastchan\npython-flatbuffers        23.5.26            pyhd8ed1ab_0    conda-forge\npython-installer          0.7.0              pyhd8ed1ab_0    conda-forge\npython-json-logger        2.0.7              pyhd8ed1ab_0    conda-forge\npython-multipart          0.0.6              pyhd8ed1ab_0    conda-forge\npython-tzdata             2023.3             pyhd8ed1ab_0    fastchan\npython-xxhash             3.2.0           py310h1fa729e_0    fastchan\npython_abi                3.10                    2_cp310    fastchan\npytorch                   2.0.0           cuda112py310he33e0d6_200    conda-forge\npytorch-cuda              11.8                 h7e8668a_5    pytorch\npytorch-lightning         2.0.6              pyhd8ed1ab_0    conda-forge\npytorch-mutex             1.0                        cuda    fastchan\npytz                      2023.3             pyhd8ed1ab_0    fastchan\npyu2f                     0.1.5              pyhd8ed1ab_0    conda-forge\npyyaml                    6.0             py310h5764c6d_4    fastchan\npyzmq                     24.0.1          py310h330234f_0    fastchan\nqt-main                   5.15.8              h7fe3ca9_15    conda-forge\nqtconsole                 5.3.1              pyhd8ed1ab_0    fastchan\nqtconsole-base            5.3.1              pyha770c72_0    fastchan\nqtpy                      2.1.0              pyhd8ed1ab_0    fastchan\nrapidfuzz                 2.15.1          py310heca2aa9_0    conda-forge\nrapids                    0.0.1              pyh9f0ad1d_0    conda-forge\nrdma-core                 28.9                 h59595ed_1    fastchan\nre2                       2023.03.02           h8c504da_0    fastchan\nreadchar                  4.0.5              pyhd8ed1ab_0    conda-forge\nreadline                  8.2                  h8228510_1    conda-forge\nreferencing               0.30.2             pyhd8ed1ab_0    fastchan\nregex                     2023.8.8        py310h2372a71_0    fastchan\nrequests                  2.31.0             pyhd8ed1ab_0    fastchan\nrequests-oauthlib         1.3.1              pyhd8ed1ab_0    conda-forge\nrequests-toolbelt         1.0.0              pyhd8ed1ab_0    fastchan\nrfc3339-validator         0.1.4              pyhd8ed1ab_0    conda-forge\nrfc3986-validator         0.1.1              pyh9f0ad1d_0    conda-forge\nrich                      13.5.1             pyhd8ed1ab_0    fastchan\nrocm-smi                  5.6.0                h59595ed_1    fastchan\nrpds-py                   0.9.2           py310hcb5633a_0    fastchan\nrsa                       4.9                pyhd8ed1ab_0    conda-forge\ns2n                       1.3.44               h06160fa_0    fastchan\nsacremoses                master                     py_0    huggingface\nsafetensors               0.3.2           py310hcb5633a_0    fastchan\nscikit-learn              1.3.0           py310hf7d194e_0    fastchan\nscipy                     1.11.2          py310ha4c1d20_0    fastchan\nsecretstorage             3.3.3           py310hff52083_1    conda-forge\nsend2trash                1.8.0              pyhd8ed1ab_0    fastchan\nsentry-sdk                1.29.2             pyhd8ed1ab_0    conda-forge\nsetproctitle              1.3.2           py310h5764c6d_1    conda-forge\nsetuptools                65.6.3             pyhd8ed1ab_0    fastchan\nshellingham               1.5.1              pyhd8ed1ab_0    fastchan\nsip                       6.7.11          py310hc6cd4ac_0    fastchan\nsix                       1.16.0             pyh6c4a22f_0    fastchan\nsleef                     3.5.1                h9b69904_2    fastchan\nsmart_open                5.2.1              pyhd8ed1ab_0    fastchan\nsmmap                     3.0.5              pyh44b312d_0    conda-forge\nsnappy                    1.1.10               h9fff704_0    fastchan\nsniffio                   1.3.0              pyhd8ed1ab_0    fastchan\nsortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge\nsoupsieve                 2.3.2.post1        pyhd8ed1ab_0    fastchan\nspacy                     3.6.1           py310hfb6f7a9_0    fastchan\nspacy-legacy              3.0.12             pyhd8ed1ab_0    fastchan\nspacy-loggers             1.0.4              pyhd8ed1ab_0    fastchan\nsrsly                     2.4.7           py310hc6cd4ac_0    fastchan\nstack_data                0.6.2              pyhd8ed1ab_0    fastchan\nstarlette                 0.27.0             pyhd8ed1ab_0    conda-forge\nstarsessions              1.3.0              pyhd8ed1ab_0    conda-forge\nsympy                     1.12            pypyh9d50eac_103    fastchan\ntbb                       2021.10.0            h00ab1b0_0    fastchan\ntblib                     1.7.0              pyhd8ed1ab_0    conda-forge\ntenacity                  8.2.2              pyhd8ed1ab_0    conda-forge\ntensorboard               2.12.3             pyhd8ed1ab_0    conda-forge\ntensorboard-data-server   0.7.0           py310h34c0648_0    conda-forge\ntensorflow                2.12.1          cuda112py310h457873b_0    conda-forge\ntensorflow-base           2.12.1          cuda112py310h622e808_0    conda-forge\ntensorflow-estimator      2.12.1          cuda112py310ha5e6de5_0    conda-forge\ntermcolor                 2.3.0              pyhd8ed1ab_0    conda-forge\nterminado                 0.15.0          py310hff52083_0    fastchan\nthinc                     8.1.11          py310hfb6f7a9_0    fastchan\nthreadpoolctl             3.2.0              pyha21a80b_0    fastchan\ntinycss2                  1.1.1              pyhd8ed1ab_0    fastchan\ntk                        8.6.12               h27826a3_0    conda-forge\ntokenizers                0.13.3          py310he1f1126_0    fastchan\ntoml                      0.10.2             pyhd8ed1ab_0    fastchan\ntomli                     2.0.1              pyhd8ed1ab_0    fastchan\ntomlkit                   0.12.1             pyha770c72_0    conda-forge\ntoolz                     0.12.0             pyhd8ed1ab_0    fastchan\ntorchaudio                2.0.0               py310_cu118    fastchan\ntorchmetrics              1.0.3              pyhd8ed1ab_0    conda-forge\ntorchvision               0.15.2          cuda112py310h0801bf5_1    conda-forge\ntornado                   6.3.2           py310h2372a71_0    fastchan\ntqdm                      4.66.1             pyhd8ed1ab_0    fastchan\ntraitlets                 5.9.0              pyhd8ed1ab_0    fastchan\ntransformers              4.31.0             pyhd8ed1ab_0    fastchan\ntranslationstring         1.4                pyh9f0ad1d_0    conda-forge\ntrove-classifiers         2023.8.7           pyhd8ed1ab_0    conda-forge\ntyper                     0.9.0              pyhd8ed1ab_0    fastchan\ntyping                    3.10.0.0           pyhd8ed1ab_0    fastchan\ntyping-extensions         4.5.0                hd8ed1ab_0    fastchan\ntyping_extensions         4.5.0              pyha770c72_0    fastchan\ntyping_utils              0.1.0              pyhd8ed1ab_0    conda-forge\ntzdata                    2023c                h71feb2d_0    conda-forge\nucx                       1.14.1               h4a2ce2d_1    fastchan\nunicodedata2              15.0.0          py310h5764c6d_0    fastchan\nuri-template              1.3.0              pyhd8ed1ab_0    conda-forge\nurllib3                   1.26.15            pyhd8ed1ab_0    fastchan\nuvicorn                   0.23.2          py310hff52083_0    conda-forge\nvenusian                  3.0.0                      py_0    conda-forge\nvirtualenv                20.24.1            pyhd8ed1ab_0    conda-forge\nwandb                     0.15.8             pyhd8ed1ab_0    conda-forge\nwasabi                    1.1.2           py310hff52083_0    fastchan\nwcwidth                   0.2.6              pyhd8ed1ab_0    fastchan\nwebcolors                 1.13               pyhd8ed1ab_0    conda-forge\nwebencodings              0.5.1                      py_1    fastchan\nwebob                     1.8.7              pyhd8ed1ab_0    conda-forge\nwebsocket-client          1.6.1              pyhd8ed1ab_0    conda-forge\nwebsockets                11.0.3          py310h2372a71_0    conda-forge\nwerkzeug                  2.3.6              pyhd8ed1ab_0    conda-forge\nwheel                     0.41.1             pyhd8ed1ab_0    conda-forge\nwidgetsnbextension        4.0.4              pyhd8ed1ab_0    fastchan\nwrapt                     1.15.0          py310h1fa729e_0    conda-forge\nxcb-util                  0.4.0                h516909a_0    fastchan\nxcb-util-image            0.4.0                h8ee46fc_1    conda-forge\nxcb-util-keysyms          0.4.0                h516909a_0    fastchan\nxcb-util-renderutil       0.3.9                hd590300_1    conda-forge\nxcb-util-wm               0.4.1                h516909a_0    fastchan\nxkeyboard-config          2.38                 h0b41bf4_0    fastchan\nxorg-kbproto              1.0.7             h7f98852_1002    fastchan\nxorg-libice               1.1.1                hd590300_0    fastchan\nxorg-libsm                1.2.4                h7391055_0    fastchan\nxorg-libx11               1.8.6                h8ee46fc_0    fastchan\nxorg-libxau               1.0.11               hd590300_0    fastchan\nxorg-libxdmcp             1.1.3                h7f98852_0    fastchan\nxorg-libxext              1.3.4                h7f98852_1    fastchan\nxorg-libxrender           0.9.11               hd590300_0    fastchan\nxorg-renderproto          0.11.1            h7f98852_1002    fastchan\nxorg-xextproto            7.3.0             h7f98852_1002    fastchan\nxorg-xf86vidmodeproto     2.3.1             h7f98852_1002    conda-forge\nxorg-xproto               7.0.31            h7f98852_1007    fastchan\nxxhash                    0.8.1                h0b41bf4_0    fastchan\nxyzservices               2023.7.0           pyhd8ed1ab_0    conda-forge\nxz                        5.2.6                h166bdaf_0    conda-forge\nyaml                      0.2.5                h7f98852_2    fastchan\nyarl                      1.9.2           py310h2372a71_0    fastchan\nzeromq                    4.3.4                h9c3ff4c_1    fastchan\nzict                      3.0.0              pyhd8ed1ab_0    conda-forge\nzipp                      3.16.2             pyhd8ed1ab_0    fastchan\nzlib                      1.2.13               hd590300_5    conda-forge\nzope.deprecation          4.4.0                      py_0    conda-forge\nzope.interface            6.0             py310h1fa729e_0    conda-forge\nzstd                      1.5.2                h8a70e8d_1    fastchan"
  },
  {
    "objectID": "compute.html",
    "href": "compute.html",
    "title": "Compute",
    "section": "",
    "text": "MLeRP is split into two regions which users can choose between - one based in Monash (Melbourne, Victoria) and one based in QCIF (Brisbane, Queensland). The clusters have seperate file systems and quotas are provisioned seperately, so you will have to transfer your files across if you’d like to switch regions."
  },
  {
    "objectID": "compute.html#monash",
    "href": "compute.html#monash",
    "title": "Compute",
    "section": "Monash",
    "text": "Monash\nThe Monash region is designed to be flexible. It is split into two partitions: BigCats and HouseCats.\n\n\nBigCats Partition\n\nCPU notebooks that control Dask workers\nLion QoS Notebook with Cheetah QoS Workers\nIdeal for:\n\nData processing\nRapid and flexible iteration during development\nExperimenting with techniques\n\nSee our tutorials for more details and this FAQ for advice on requesting resources through the Dask scheduler.\n\n\nBatch submission\nLion QoS Script\nIdeal for:\n\nModel training\nHeavy duty processing\nHyperparameter sweeps\n\n\n\n\nHouseCats Partition\n\nNotebooks with GPU compute\nTabby QoS Notebook\nIdeal for:\n\nData exploration\nData visualization\nNew users who just want to get started with minimal setup\n\nMonash workloads are also able to make use of attached NVME for temporary file storage."
  },
  {
    "objectID": "compute.html#qcif",
    "href": "compute.html#qcif",
    "title": "Compute",
    "section": "QCIF",
    "text": "QCIF\nThe QCIF region uses a more traditional approach with A100 GPU enabled notebook or terminal sessions. This user experience is much closer to using a service like Google Colab, but with a permanent file system.\nGPU notebook or terminal sessions in three sizes:\n\nRegular\n\n20 VCPUs\n190 GB RAM\n10 GB VRAM\n\nDouble\n\n20 VCPUs\n190 GB RAM\n20 GB VRAM\n\nNode\n\n60 VCPUs\n570 GB RAM\n2x 10 GB VRAM + 1x 20 GB VRAM"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the MLeRP User Guide",
    "section": "",
    "text": "Artwork by Kiowa Scott-Hurly\n\n\nThe MLeRP environment creates a middle ground that has the interactivity of a notebook with the power of a HPC environment that can share valuable resources between other users while code isn’t being executed.\nWe provide users with CPU based Jupyter notebook sessions capable of basic analysis, with the ability to interactively send jobs to a SLURM queue for GPU or parallelised CPU acceleration through Dask.\nBe sure to check out the Jupyter, SLURM and Dask documentation for more information.\nMLeRP at the moment is only available through closed beta but is planned to be available through open beta to all Australian academic researchers through AAF. You can sign up to our mailing list if you’d like us to let you know when we’re ready for open beta.\nMLeRP has been made possible through ARDC funding. Thanks ARDC!\n\n\nLog in\n\n\nSign up\n\n\n\n\nContact us\nWant to get in touch with the developers? You can reach us at mlerphelp@gmail.com."
  },
  {
    "objectID": "tutorials/2_dask_pytorch.html",
    "href": "tutorials/2_dask_pytorch.html",
    "title": "PyTorch and Dask",
    "section": "",
    "text": "Defining models, datasets and functions\nIf you’re doing something relatively simple, Dask has integrations with Scikit-Learn and XGBoost. You can also pass PyTorch models into Scikit-Learn with Skorch and TensorFlow models with SciKeras.\nBut if you need to do something more complex, Dask clusters can have python functions submitted to them to remotely execute code. This gives us the low level control to implement whatever bespoke algorithm we want and have it accelerated by remote GPUs.\nIn this example we’re going to write our own PyTorch functions to train a custom model on the CIFAR dataset. While we could do this with Skorch, we hope that this example gives you some idea of how Dask can be flexible enough for any applications that you need.\nContent adapted from the PyTorch CIFAR10 Tutorial\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.multiprocessing as mp\n\n# Define data transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n# Define dataset and dataloader\nbatch_size = 1024\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\nvalidset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                        download=True, transform=transform)\n\n# Note that we need to set the multiprocessing context so that PyTorch doesn't get\n# PyTorch likes to use 'forking' while Dask uses 'spawn'\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=16, multiprocessing_context=mp.get_context(\"fork\"))\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n                                          shuffle=True, num_workers=16, multiprocessing_context=mp.get_context(\"fork\"))\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n/apps/mambaforge/envs/dsks/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\nNote that this cell may warn us that there is a mismatch between our requested resources and the number of worker processes. This is ok, as we have sized this DataLoader to match the Dask worker that we request later on.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define a simple conv net\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n        self.conv6 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(4 * 4 * 64, 4 * 64)\n        self.fc2 = nn.Linear(4 * 64, 64)\n        self.fc3 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.relu(self.conv6(x))\n        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nThis train function will load any saved state for the provided model, then train for a number of epochs. When its done it will then save the state and return the average loss of the last epoch.\nThere are also a few flags which have been included to show the printing and error handling behaviour of dask clusters.\n\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm\n\n\n# loader: train dataloader\n# arch: model archetechture for training\n# path: model path for load and save\n# load: whether to load model from path\n# save: whether to save model to path\n# test: only run one batch for testing\n# error: throw an assertion error\n# return: average loss of epoch or loss of one batch if testing\ndef train(loader, arch=Net, path=\"./model\", epochs=1, load=False, save=True, test=False, error=False):\n    model = arch()\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Load state from disk so that we can split up the job\n    if load: \n        state = torch.load(path, map_location=\"cpu\")\n        model.load_state_dict(state[\"model\"])\n        model.to(device)\n        optimizer.load_state_dict(state[\"optimizer\"])\n    else:\n        model.to(device)\n    \n    # A typical PyTorch training loop\n    model.train()\n    for _ in range(epochs):\n        running_loss = 0\n        \n        for i, (inputs, labels) in enumerate(loader):\n            # put the inputs on the device\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.detach().item()\n            \n            # Force an error\n            if error:\n                assert 0 == 1\n            \n            # Stop after one batch when testing        \n            if test: \n                print(\"When running in a local cluster you can see print statements\")\n                break\n    \n    # Save model after each epoch\n    if save:\n        torch.save({\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict()\n            }, path)\n    \n    return running_loss / len(loader) if not test else loss.detach().item()\n\nThis valid function will load the state of the model we’ve defined, then calculate the average loss and accuracy over the dataset.\n\n# loader: train dataloader\n# arch: model archetechture for validating\n# path: model path for load and save\n# return: average loss and accuracy of epoch\ndef valid(loader, arch=Net, path=\"./model\"):\n    # Initialise device\n    model = arch()\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Load state from disk so that we can split up the job\n    state = torch.load(path, map_location=\"cpu\")\n    model.load_state_dict(state[\"model\"])\n    model.to(device)\n    model.eval()\n    \n    # A typical PyTorch validation loop\n    running_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(loader):\n            # put the inputs on the device\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # forward\n            outputs = model(inputs)\n            \n            # loss\n            loss = criterion(outputs, labels)\n            running_loss += loss.detach().item()\n            \n            # accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return running_loss / len(loader), correct / total\n\n\n\nTesting with a LocalCluster\nDask LocalClusters are easiest to use interactive development. This will mean that code will execute in the notebook session allowing you to view print statements and debug errors normally rather than dealing with remote code execusion before we’re ready. Later, when you are satisfied with your code you can switch over to a SLURMCluster to accelerate with GPU.\nDask prefers to control all processes so that it can manage them more gracefully if they fail, but we need to give PyTorch the control to use multiprocessing as needed. To do this set proccesses=False to allow for multiprocessing inside Dask jobs.\n\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster(processes=False)\nclient = Client(cluster)\n\nWe can submit our function to the cluster with the client.submit method. This will return a future which can be unpacked with its result using future.result(). We can see the outputs of print statements while we’re using a LocalCluster. Print statements will not be visible when executing remotely with SLURMCluster.\n\nfuture = client.submit(train, trainloader, test=True)\nfuture.result()\n\n/apps/mambaforge/envs/dsks/lib/python3.10/site-packages/distributed/client.py:3160: UserWarning: Sending large graph of size 146.58 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n/apps/mambaforge/envs/dsks/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\nWhen running in a local cluster you can see print statements\n\n\n2.3079872131347656\n\n\nWe can use the client.scatter method to scatter large objects out to our workers ahead of time for more efficient execution.\n\ntrainloader_future = client.scatter(trainloader)\nclient.submit(train, trainloader, test=True).result()\n\nWhen running in a local cluster you can see print statements\n\n\n2.302961826324463\n\n\n\nclient.shutdown()\n\n\n\nTraining with a SLURMCluster\nDask usually uses a ‘nanny’ that monitors any worker processes and gracefully restarts them if they fail or are killed while performing computations. The nanny is not compatable with daemonic processes - that is dask workers cannot perform multiprocessing while it’s being used. We therefore need to set nanny=False to turn off the nanny to allow for multiprocessing inside Dask jobs for the cluster to work with PyTorch. (Just like when we processes=False for the LocalCluster.)\nWe can pass in extra SLURM requirements in job_extra_directives to request a GPU for our jobs. To read more about configuring the SLURMCluster to interact with the SLURM queue, go to Dask’s jobqueue documentation.\n\nfrom dask_jobqueue import SLURMCluster\nfrom distributed import Client\ncluster = SLURMCluster(\n    memory=\"128g\", processes=1, cores=16, job_extra_directives=[\"--gres=gpu:1\"], nanny=False\n)\n\ncluster.scale(1)\nclient = Client(cluster)\n\nSince this code is executing remotely we won’t see our print statements\n\ntrainloader_future = client.scatter(trainloader)\nclient.submit(train, trainloader_future, test=True).result()\n\n2.305359363555908\n\n\nDask will raise any errors that the process triggers locally, even when executing remotely\n\ntrainloader_future = client.scatter(trainloader)\nclient.submit(train, trainloader_future, error=True).result()\n\nAssertionError: \n\n\nFinally we can bring everything together and run our training loop.\n\n# Run the training loop\nepochs = 5\ntrainloader_future = client.scatter(trainloader)\nvalidloader_future = client.scatter(validloader)\nwith tqdm(total=(epochs)) as pbar:\n    for epoch in range(epochs):\n        train_loss = client.submit(train, trainloader_future, load=(epoch &gt; 0)).result()\n        valid_loss, accuracy = client.submit(valid, validloader_future).result()\n        pbar.update()\n        pbar.set_postfix(loss=train_loss)\n        print( f\"epoch: {epoch}, train_loss: {train_loss : .3f}, valid_loss: {valid_loss : .3f}, accuracy: {accuracy : .3f}\")\n\n\n\n\nepoch: 0, train_loss:  2.285, valid_loss:  2.160, accuracy:  0.204\nepoch: 1, train_loss:  2.025, valid_loss:  1.952, accuracy:  0.275\nepoch: 2, train_loss:  1.915, valid_loss:  1.852, accuracy:  0.323\nepoch: 3, train_loss:  1.826, valid_loss:  1.786, accuracy:  0.350\nepoch: 4, train_loss:  1.759, valid_loss:  1.727, accuracy:  0.366\n\n\n\n\nMeasuring Dask’s overhead\nOffloading tasks to Dask doesn’t come for free, there is an initial cost associated with sending the data to a remote device. Let’s compare the time it would take to train a Resnet18 on CIFAR for a range of epochs comparing a local GPU, a remote GPU using Dask and a remote GPU using Dask with a scattered dataset. For this expriment we will not bother saving the weights afterwards since this should be relatively constant between methods.\nNote that this test was run directly on the compute node to gain direct access to the GPUs to measure overheads. You will only be able to mimic our results for the final graph if you’re running with an inbuilt GPU (Tabby service) since it compares reserved GPUs with Dask driven GPU jobs. Running this cell in a lion service will likely freeze the notebook since you’d have no accelleration.\n\nfrom torchvision.models import resnet18\nfrom time import time\n\n# Store times in arrays\nlocal = []\nremote = []\nscatter = []\n\n# Test some number of epochs\nepoch_list = [1, 2, 3, 5, 10]\nwith tqdm(total=(len(epoch_list) * 3)) as pbar:\n    for num_epochs in epoch_list:\n        \n        # Local GPU\n        start = time()\n        train(trainloader, arch=resnet18, epochs=(num_epochs + 1), save=False)\n        local.append(time() - start)\n        pbar.update()\n        \n        # Remote GPU\n        start = time()\n        client.submit(train, trainloader, arch=resnet18, epochs=(num_epochs + 1), save=False).result()\n        remote.append(time() - start)\n        pbar.update()\n        \n        # Remote GPU with scatter\n        start = time()\n        trainloader_future = client.scatter(trainloader)\n        client.submit(train, trainloader_future, arch=resnet18, epochs=(num_epochs + 1), save=False).result()\n        scatter.append(time() - start)\n        pbar.update()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\ndata = list(chain(*zip(local, remote, scatter)))\ncolumns = []\nfor num_epochs in epoch_list:\n    for test in [\"local\", \"remote\", \"scatter\"]:\n        columns.append(test + \" \" + str(num_epochs))\n\nplt.bar(range(len(data)), data, tick_label=columns)\nplt.xticks(rotation=90)\nplt.xlabel(\"Experiment\")\nplt.ylabel(\"Seconds\")\nplt.title(\"Runtime comparison for local, remote and scatter\")\nplt.show()\n\n\n\n\nFrom this experiment we can see that the cost associated with running code remotely is small, and the impact decreases with the size of the function that we submit. It also shows that it always makes sense to scatter large objects before computing, even for small jobs.\n\nclient.shutdown()"
  }
]